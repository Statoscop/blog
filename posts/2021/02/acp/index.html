<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Le blog de Statoscop</title>
  <link rel="icon" type="image/x-icon" href="/theme/images/favicon.ico" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="/posts/2021/02/acp/" rel="canonical" />
  <!-- Feed -->

  <link href="/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="/theme/css/code_blocks/github.css" rel="stylesheet">

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="Rien à voir, c'est juste un test">

    <meta name="author" content="Antoine Sireyjol">

    <meta name="tags" content="python">
    <meta name="tags" content="scikit-learn">
    <meta name="tags" content="clustering">




<!-- Open Graph -->
<meta property="og:site_name" content="Le blog"/>
<meta property="og:title" content="ACP bébé"/>
<meta property="og:description" content="Rien à voir, c'est juste un test"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/posts/2021/02/acp/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2021-02-01 11:15:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/antoine-sireyjol.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="scikit-learn"/>
<meta property="article:tag" content="clustering"/>
<meta property="og:image" content="/theme/images/post-bg.jpg">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "ACP bébé",
  "headline": "ACP bébé",
  "datePublished": "2021-02-01 11:15:00+01:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Antoine Sireyjol",
    "url": "/author/antoine-sireyjol.html"
  },
  "image": "/theme/images/post-bg.jpg",
  "url": "/posts/2021/02/acp/",
  "description": "Rien à voir, c'est juste un test"
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" >
      <div class="inner">
        <nav id="navigation">
          <span class="blog-logo">
            <a href="https://statoscop.fr"><img src="/theme/images/logo-statoscop.png" alt="Blog Logo" /></a>
          </span>
          <span id="menu-button" class="nav-button">
            <a href="/">Retour à l'accueil du blog</a>
          </span>
        </nav>
        <h1 class="post-title">ACP bébé</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="/author/antoine-sireyjol.html">Antoine Sireyjol</a>
            | <time datetime="lun. 01 février 2021">lun. 01 février 2021</time>
        </span>
        <!-- TODO : Modified check -->
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <div class="toc"><span class="toctitle">Table of contents:</span><ul>
<li><a href="#analyse-en-composantes-principales">Analyse en composantes principales</a><ul>
<li><a href="#explication-introductive">Explication introductive</a></li>
<li><a href="#premier-exemple-en-deux-dimensions">Premier exemple en deux dimensions</a></li>
<li><a href="#reduction-du-nombre-de-dimensions">Réduction du nombre de dimensions</a></li>
<li><a href="#cas-pratique-sur-les-chiffres-ecrits-a-la-main">Cas pratique sur  les chiffres écrits à la main</a></li>
<li><a href="#choisir-le-nombre-de-composantes">Choisir le nombre de composantes</a></li>
<li><a href="#cas-pratique-1-acp-sur-iris">Cas pratique 1 : ACP sur iris</a></li>
<li><a href="#cas-pratique-2-classification-des-vins">Cas pratique 2 : classification des vins</a></li>
<li><a href="#cas-pratique-3-modele-de-prediction-sur-mnist">Cas pratique 3 : modèle de prédiction sur MNIST</a></li>
</ul>
</li>
</ul>
</div>
<p>[[embed url=http://www.youtube.com/watch?v=6YbBmqUnoQM]]</p>
<h1 id="analyse-en-composantes-principales">Analyse en composantes principales</h1>
<p>Ce notebook reprend une partie des exemples du chapitre du Python Data Science Handbook sur l'analyse en composantes principales écrit par Jake VanderPlas et disponible ici : https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html</p>
<h2 id="explication-introductive">Explication introductive</h2>
<p>L'analyse en composantes principales est une méthode consistant à transformer des variables corrélées entre elles en nouvelles variables. Chacune de ces nouvelles variables est le résultat d'une combinaison linéaire des anciennes variables. Ces nouvelles variables sont appelées composantes principales et sont décorrélées les unes des autres. Leur nombre est inférieur ou égal au nombre de variables à l'origine. Cette méthode est donc utilisée en particulier pour réduire le nombre de dimensions (= de variables) d'une problématique donnée. </p>
<p>On commence par importer les modules nécessaires : </p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</code></pre></div>

<h2 id="premier-exemple-en-deux-dimensions">Premier exemple en deux dimensions</h2>
<p>On reprend les variables créées par Jake VanderPlas pour son exemple en 2 dimensions : </p>
<div class="highlight"><pre><span></span><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_6_0.png"></p>
<p>Les variables X et Y sont clairement liées par une relation linéaire, mais ça n'est pas ce qui nous intéresse forcément en ACP. On va chercher à représenter comment les points sont distribués en fonction des valeurs de x et y.</p>
<p>Commençons par construire les 2 premières composantes des valeurs de X :</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=2, random_state=None,</span>
<span class="err">    svd_solver=&#39;auto&#39;, tol=0.0, whiten=False)</span>
</code></pre></div>

<p>L'objet pca une fois adapté ("fité") aux données contient deux objets qui nous intéressent particulièrement :<br>
- les composantes<br>
- la variance expliquée</p>
<p>On peut retrouver ces éléments ainsi : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[-0.94446029, -0.32862557],</span>
<span class="err">       [-0.32862557,  0.94446029]])</span>
</code></pre></div>

<p>On interprète ces composantes ainsi : <br>
PC1 = -0.94 * x1 - 0.32 * x2<br>
PC2 = -0.32 * x1 + 0.94 * x2</p>
<p>Pour la variance, on peut sortir la variance expliquée par les composantes principales et la part de la variance totale expliquée par ces composantes : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0.7625315, 0.0184779])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">0.7771043494141933</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0.97634101, 0.02365899])</span>
</code></pre></div>

<p>Ce ratio représente la contribution de chaque composante à l'explication de la variance globale. On remarque ici qu'une seule composante principale permet d'expliquer 97% de la variance globale. Comme on a utilisé autant de composantes principales que de variables disponibles, le ratio somme à 1.  </p>
<p>Les composantes principales peuvent être représentées comme des nouveaux axes, que Jake VanderPlas représnete ainsi dans son article, la longueur des vecteurs étant associée à l'importance de la variance expliquée par chacune des composantes :  </p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span>
                    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span>
                    <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrowprops</span><span class="p">)</span>

<span class="c1"># plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">draw_vector</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_18_0.png"></p>
<p>Le premier axe est défini en fonction de la direction de la plus grande variance, le second axe (qui doit être orthogonal au premier) en fonction de la direction de la seconde plus grande variance (et caetera quand il y a plus de dimensions).<br>
On peut également représenter graphiquement les points dans un nouveaux système de coordonnées où les axes sont les composantes principales. Comme les composantes principales sont définies de manière à ne pas être corrélées entre elles, la représentation graphique ne laisse pas apparaître de liens de corrélation entre celles-ci : </p>
<div class="highlight"><pre><span></span><code><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Composantes principales&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">Text(0.5, 1.0, &#39;Composantes principales&#39;)</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_20_1.png"></p>
<p>On profite d'avoir créé <code>X_pca</code> pour vérifier que les axes sont bien décorrélés : </p>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[ 1.00000000e+00, -5.12304892e-16],</span>
<span class="err">       [-5.12304892e-16,  1.00000000e+00]])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">X_pca</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0.67676923, 0.0597386 ])</span>
</code></pre></div>

<h2 id="reduction-du-nombre-de-dimensions">Réduction du nombre de dimensions</h2>
<p>L'ACP est surtout intéressante pour réduire le nombre de dimensions lors d'une analyse tout en conservant le plus de variance expliquée de nos observations. On peut le faire par exemple dans notre cas en ne retenant qu'une seule composante principale : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original shape:   &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transformed shape:&quot;</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">original shape:    (200, 2)</span>
<span class="err">transformed shape: (200, 1)</span>
</code></pre></div>

<p>Ce qui donne en représentation graphique : </p>
<div class="highlight"><pre><span></span><code><span class="n">X_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_27_0.png"></p>
<h2 id="cas-pratique-sur-les-chiffres-ecrits-a-la-main">Cas pratique sur  les chiffres écrits à la main</h2>
<p>Vous pouvez charger les données ainsi : </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">data</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],</span>
<span class="err">       [ 0.,  0.,  0., ..., 10.,  0.,  0.],</span>
<span class="err">       [ 0.,  0.,  0., ..., 16.,  9.,  0.],</span>
<span class="err">       ...,</span>
<span class="err">       [ 0.,  0.,  1., ...,  6.,  0.,  0.],</span>
<span class="err">       [ 0.,  0.,  2., ..., 12.,  0.,  0.],</span>
<span class="err">       [ 0.,  0., 10., ..., 12.,  1.,  0.]])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">(1797, 64)</span>
</code></pre></div>

<p><strong>À vous</strong> :<br>
- Faites une analyse en composantes principales avec 2 composantes principales<br>
- Donnez la part de variance expliquée par ces deux composantes<br>
- Représentez les données en fonction de ces composantes principales. Cette analyse permet-elle de séparer correctement les différentes classes? (Rappel : les numéros corrects sont donnés par digits.target)          <br>
- <br>
- 
- <br>
- <br>
- <br>
- <br>
- 
- <br>
- <br>
- 
-  <br>
-    </p>
<p><strong>On corrige ensemble</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># project from 64 to 2 dimensions</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="c1"># pca.fit_transform = pca.fit() + pca.transform()</span>
</code></pre></div>

<p>Les ratios de variance expliquée sont les suivants : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0.14890594, 0.13618771])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">0.28509364823699085</span>
</code></pre></div>

<p>Soit moins de 30% de la variance totale expliquée avec les deux premières composantes... </p>
<p>Les composantes sont les suivantes (juste les 6 premières colonnes) : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[ 6.78177507e-18, -1.73094639e-02, -2.23428828e-01,</span>
<span class="err">        -1.35913306e-01, -3.30323109e-02, -9.66340806e-02],</span>
<span class="err">       [-1.56589325e-17, -1.01064568e-02, -4.90849204e-02,</span>
<span class="err">        -9.43338268e-03, -5.36015799e-02, -1.17755315e-01]])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;prism&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">&lt;matplotlib.colorbar.Colorbar at 0x142c6705518&gt;</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_39_1.png"></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Qu&#39;est-ce qu&#39;on a si on fait la même représentation sur </span>
<span class="c1"># 2 variables au hasard?</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;prism&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;variable 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;variable 3&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">&lt;matplotlib.colorbar.Colorbar at 0x142c6614a90&gt;</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_40_1.png"></p>
<p>On voit que l'ACP permet en partie de séparer les différentes classes mais probablement pas assez pour permettre une bonne classification dans ce repère de données.  Dans son article, Jake VanderPlas montre qu'avec une ACP de 8 composantes principales on peut reconstruire les représentations des chiffres quasiment à l'identique (donc avec 8 fois moins de variables!). C'est donc une solution possible pour accélérer considérablement le temps d'exécution d'algorithmes de machine learning.</p>
<h2 id="choisir-le-nombre-de-composantes">Choisir le nombre de composantes</h2>
<p>Si choisir deux composantes principales est toujours intéressant pour pouvoir représenter graphiquement ses résultats, il est important de tenir compte de la variance expliquée par chaque composante afin de déterminer quel nombre retenir.  </p>
<p>On peut pour cela représenter l'évolution du ratio de la variance expliquée en fonction du nombre de composantes retenues :  </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;nombre de composantes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;variance expliquée&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_43_0.png"></p>
<p>On peut avec la fonction <code>PCA</code> définir le nombre de composantes voulues mais aussi la variance minimum à atteindre : </p>
<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">11</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">0.7619490686562893</span>
</code></pre></div>

<h2 id="cas-pratique-1-acp-sur-iris">Cas pratique 1 : ACP sur iris</h2>
<ul>
<li>Importez la base de données iris disponible dans sklearn.datasets</li>
<li>Avant de mener notre ACP, il faudrait transformer d'abord nos données. Voyez-vous de quelle manière?</li>
<li>Faites une ACP avec deux composantes et représentez les points en fonction de ces composantes.  </li>
<li>Les différentes espèces de fleurs sont-elles bien partitionnées dans ce plan?  </li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Import données</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">values</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">target_names</span>

<span class="c1"># On paramètre notre PCA</span>
<span class="n">pca</span><span class="o">=</span><span class="n">PCA</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca_iris</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">0.9776852063187949</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># On fait le plot</span>

<span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pca_iris</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_iris</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="n">target_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_50_0.png"></p>
<h2 id="cas-pratique-2-classification-des-vins">Cas pratique 2 : classification des vins</h2>
<ul>
<li>Importez la base de données wine disponible dans sklearn.datasets   </li>
<li>Faites le graphique de l'évolution du ratio de la variance expliquée en fonction du nombre de composantes principales. Y-a-t-il matière à réduire le nombre de dimensions? Quel nombre de composantes principales choisiriez-vous?  </li>
<li>Faites une ACP et représentez les points en fonction de ces composantes.  </li>
<li>Les différentes classes sont-elles bien partitionnées dans ce plan?  </li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Import données</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_wine</span>

<span class="n">values</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">load_wine</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">load_wine</span><span class="p">()</span><span class="o">.</span><span class="n">target_names</span>

<span class="c1"># on standardise nos données : </span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">values_cr</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># On paramètre notre PCA pour qu&#39;il y ait 80% de la variance expliquée</span>
<span class="n">pca</span><span class="o">=</span><span class="n">PCA</span><span class="p">(</span><span class="mf">0.80</span><span class="p">)</span>
<span class="n">pca_wine</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">values_cr</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1">---------------------------------------------------------------------------</span>

<span class="n">NameError</span>                                 <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="k">call</span> <span class="k">last</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="k">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">a2c2529e71c4</span><span class="o">&gt;</span> <span class="k">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span><span class="p">()</span>
     <span class="mi">10</span> 
     <span class="mi">11</span> <span class="o">#</span> <span class="k">On</span> <span class="n">paramètre</span> <span class="n">notre</span> <span class="n">PCA</span> <span class="n">pour</span> <span class="n">qu</span><span class="s1">&#39;il y ait 80% de la variance expliquée</span>
<span class="s1">---&gt; 12 pca=PCA(0.80)</span>
<span class="s1">     13 pca_wine = pca.fit_transform(values_cr)</span>


<span class="s1">NameError: name &#39;</span><span class="n">PCA</span><span class="err">&#39;</span> <span class="k">is</span> <span class="k">not</span> <span class="k">defined</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">5</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">0.8016229275554788</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">13</span>
</code></pre></div>

<p>On explique 80% de la variance avec 5 composantes principales, et il y a 13 variables dans le datasets initial.</p>
<p>Voyons comment les deux premières composantes nous permettent de représenter les données : </p>
<div class="highlight"><pre><span></span><code><span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pca_wine</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_wine</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="n">target_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_57_0.png"></p>
<h2 id="cas-pratique-3-modele-de-prediction-sur-mnist">Cas pratique 3 : modèle de prédiction sur MNIST</h2>
<ul>
<li>Sur les données MNIST, choisissez le nombre de composantes principales permettant de garder 75% de la variance totale.  </li>
<li>Faites tourner un modèle de prédiction de régression logistique comme celui que vous aviez fait avec Louis.  </li>
<li>Comparez les résultats en termes de précision.  </li>
<li>Comparez les résultats en termes de temps d'exécution</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_wine</span>
<span class="n">features</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">load_wine</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">target</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="err">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="err">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="err">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,</span>
<span class="err">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="err">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="err">       2, 2])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">features</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">target</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="err">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="err">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="err">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="err">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
</code></pre></div>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=ACP bébé&amp;url=/posts/2021/02/acp/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/posts/2021/02/acp/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=/posts/2021/02/acp/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="/tag/python.html">python</a><a href="/tag/scikit-learn.html">scikit-learn</a><a href="/tag/clustering.html">clustering</a>                </aside>

                <div class="clear"></div>


                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Blog réalisé par les fondateurs de Statoscop et publié avec
            <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a>
          </span>
          <span class="credits-software">
            Thème <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a>
          </span>
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="/theme/js/script.js"></script>

</body>
</html>