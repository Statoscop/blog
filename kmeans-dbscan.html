<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Le blog de Statoscop</title>
  <link rel="icon" type="image/x-icon" href="https://statoscop.github.io/blog/theme/images/favicon.ico" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="https://statoscop.github.io/blog/kmeans-dbscan.html" rel="canonical" />
  <!-- Feed -->
        <link href="https://statoscop.github.io/blog/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Le blog Full Atom Feed" />
          <link href="https://statoscop.github.io/blog/feeds/{slug}.atom.xml" type="application/atom+xml" rel="alternate" title="Le blog Categories Atom Feed" />

  <link href="https://statoscop.github.io/blog/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="https://statoscop.github.io/blog/theme/css/code_blocks/tomorrow.css" rel="stylesheet">

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="C'est a propos de k-means et dbscan">

    <meta name="author" content="Louis Kuhn">

    <meta name="tags" content="python">
    <meta name="tags" content="scikit-learn">
    <meta name="tags" content="clustering">




<!-- Open Graph -->
<meta property="og:site_name" content="Le blog"/>
<meta property="og:title" content="K-means et DBSCAN"/>
<meta property="og:description" content="C'est a propos de k-means et dbscan"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://statoscop.github.io/blog/kmeans-dbscan.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2021-02-01 11:58:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://statoscop.github.io/blog/author/louis-kuhn.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="scikit-learn"/>
<meta property="article:tag" content="clustering"/>
<meta property="og:image" content="https://statoscop.github.io/blog/theme/images/post-bg.jpg">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "K-means et DBSCAN",
  "headline": "K-means et DBSCAN",
  "datePublished": "2021-02-01 11:58:00+01:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Louis Kuhn",
    "url": "https://statoscop.github.io/blog/author/louis-kuhn.html"
  },
  "image": "https://statoscop.github.io/blog/theme/images/post-bg.jpg",
  "url": "https://statoscop.github.io/blog/kmeans-dbscan.html",
  "description": "C'est a propos de k-means et dbscan"
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>


    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" >
      <div class="inner">
        <nav id="navigation">
          <span class="blog-logo">
            <a href="https://statoscop.fr"><img src="https://statoscop.github.io/blog/theme/images/logo-statoscop.png" alt="Blog Logo" /></a>
          </span>
          <span id="menu-button" class="nav-button">
            <a href="https://statoscop.github.io/blog">Retour à l'accueil du blog</a>
          </span>
        </nav>
        <h1 class="post-title">K-means et DBSCAN</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="https://statoscop.github.io/blog/author/louis-kuhn.html">Louis Kuhn</a>
            | <time datetime="lun. 01 février 2021">lun. 01 février 2021</time>
        </span>
        <!-- TODO : Modified check -->
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <h1 id="classification-en-k-moyennes">Classification en k - moyennes</h1>
<p>Ce type de classification non supervisée est un algorithme de machine learning très utile pour classifier rapidement des bases de données volumineuses. En effet, plutôt que de calculer les distances de l'ensemble des points entre eux, il va procéder ainsi :<br>
- Il initialise un nombre de centroïdes (de classes) qu'il va placer dans l'espace des points de manière aléatoire. <br>
- Il associe ensuite à chaque centroïde les points qui lui sont les plus proches, créant ainsi autant de classes que de centroïdes.<br>
- Il déplace ensuite les centroïdes au centre de gravité de leur classe.<br>
- Il répète les étapes précédentes jusqu'à la convergence du modèle.  </p>
<p>Comme une illustration vaut mieux que trop d'explications, on peut regarder sur cette vidéo comment fonctionne l'algorithme : </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;5I3Ei69I40s&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>

<p><iframe
    width="800"
    height="300"
    src="https://www.youtube.com/embed/5I3Ei69I40s"
    frameborder="0"
    allowfullscreen
></iframe>
</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_kmeans_algorithm</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_2_0.png"></p>
<p>Pour un nombre de classes donné, cet algorithme cherche à <strong>minimiser la variance intra-classe</strong> et à <strong>maximiser la variance inter-classes</strong>. </p>
<h2 id="k-means-avec-python">k-means avec Python</h2>
<p>Sur Python, on va utiliser la fonction Kmeans de scikit-learn. Elle propose de nombreux paramètres mais le seul qui nous intéresse pour l'instant c'est <code>n_clusters</code>, le nombre de classes voulu. On ajuste notre modèle sur nos données avec <code>.fit</code> et on obtient nos classes avec <code>. predict</code>. </p>
<p><strong>À vous!</strong><br>
- Faites tourner un modèle k-means sur les données <code>wine</code> disponibles dans scikit learn pour essayer de retrouver les 3 classes de vin. <br>
- Comparez vos résultats avec la vraie classification. L'algorithme a-t-il su partitionner correctement les données?  </p>
<h2 id="-">-</h2>
<h2 id="-_1">-</h2>
<h2 id="-_2">-</h2>
<h2 id="-_3">-</h2>
<h2 id="-_4">-</h2>
<h2 id="-_5">-</h2>
<p><strong>On corrige ensemble</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Import des librairies et base :</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_wine</span>

<span class="n">features</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">load_wine</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Préparation des données : on centre-réduit</span>
<span class="n">sc_x</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_cr</span> <span class="o">=</span> <span class="n">sc_x</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Pour montrer ce qu&#39;il se passe quand on ne centre pas :</span>
<span class="c1"># X_cr = features</span>

<span class="c1"># modele k-means</span>
<span class="n">km_wine</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">km_wine</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>
<span class="n">classes_km</span> <span class="o">=</span> <span class="n">km_wine</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>


<span class="c1"># on vérifie avec les vraies données</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">classes_km</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
    <tr>
      <th>row_0</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>65</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>3</td>
      <td>48</td>
    </tr>
    <tr>
      <th>2</th>
      <td>59</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="c1"># On peut aussi voir la matrice de confusion</span>
<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">classes_km</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[ 0,  3, 48],</span>
<span class="err">       [59,  3,  0],</span>
<span class="err">       [ 0, 65,  0]], dtype=int64)</span>
</code></pre></div>

<h2 id="choisir-le-nombre-de-classes">Choisir le nombre de classes</h2>
<p>On voit bien que la convergence ou non de l'algorithme tient aussi bien aux données qu'au nombre de classes que l'on choisit. De plus, même si l'algorithme converge il n'est pas dit que le nombre de classes choisi soit pertinent. Que se passerait-il par exemple si nous avions choisi 2 classes pour l'exemple ci-dessus?</p>
<p>Un outil indispensable pour mener une analyse en k-means est l'évolution de la distance de chaque point à son centroïde en fonction du nombre de classes choisi. Cette distance décroît avec le nombre de classes jusqu'à atteindre 0 lorsque le nombre de classes est égal au nombre d'observations. Sur Python, on obtient cette valeur en appelant <code>.inertia_</code> depuis un objet KMeans.    </p>
<p><strong>Exercice</strong><br>
Représenter l'évolution de l'inertie en fonction du nombre de classes (testez de 1 classe à 25 classes) pour le problème précédent. La classification en 3 catégories est-elle justifiée?   </p>
<h2 id="-_6">-</h2>
<h2 id="-_7">-</h2>
<h2 id="-_8">-</h2>
<h2 id="-_9">-</h2>
<h2 id="-_10">-</h2>
<h2 id="-_11">-</h2>
<p><strong>On corrige ensemble</strong> : On fait tout simplement une boucle sur le nombre de classes et on représente l'évolution de l'inertie : </p>
<div class="highlight"><pre><span></span><code><span class="n">somme_distance</span> <span class="o">=</span> <span class="p">[</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span><span class="o">.</span><span class="n">inertia_</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]</span>
<span class="n">somme_distance</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">[2314.0000000000005,</span>
<span class="err"> 1659.0079672511501,</span>
<span class="err"> 1277.928488844642,</span>
<span class="err"> 1180.4507321332258,</span>
<span class="err"> 1101.3402535169776,</span>
<span class="err"> 1047.9390550499788,</span>
<span class="err"> 1000.1918567240638,</span>
<span class="err"> 950.1636494828549,</span>
<span class="err"> 892.2714587805834,</span>
<span class="err"> 862.2202081666329,</span>
<span class="err"> 834.0184294655743,</span>
<span class="err"> 796.9749041294895,</span>
<span class="err"> 771.2485896368039,</span>
<span class="err"> 745.8601371901368,</span>
<span class="err"> 719.5549349545211,</span>
<span class="err"> 691.8665070015936,</span>
<span class="err"> 670.5256118073319,</span>
<span class="err"> 668.8906963651076,</span>
<span class="err"> 651.3676063009923,</span>
<span class="err"> 621.4589958961208,</span>
<span class="err"> 615.6903078062442,</span>
<span class="err"> 593.8327565957943,</span>
<span class="err"> 582.3290794431626,</span>
<span class="err"> 572.5724770873846,</span>
<span class="err"> 558.3590853554035]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># Représentation graphique : </span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">somme_distance</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Nombre de classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Distance des points à leur centroïde&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_15_0.png"></p>
<p>On choisit le nombre de classes au niveau du "coude" de la courbe</p>
<h2 id="limites-des-k-means">Limites des k-means</h2>
<p>La grande limite des k-means est sans doute que cet algorithme nécessite un choix a priori du nombre de classes. Il existe certes des outils comme le graphique que l'on vient de faire pour orienter notre décision mais il ne reflètera pas forcément la qualité de notre clustering.  </p>
<p>Un bon exemple de cette limite peut s'illustrer avec les données suivantes : </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="c1"># generate some random cluster data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span> <span class="n">centers</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">74</span><span class="p">)</span>
<span class="c1"># transform the data to be stretched</span>
<span class="n">transformation</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">transformation</span><span class="p">)</span>
<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_18_0.png"></p>
<p><strong>Exercice</strong> :<br>
Choisissez le nombre de classes visuellement et avec le graphique de l'évolution de l'inertie. Les deux manières correspondent-elles?<br>
Représentez graphiquement la classification obtenue avec l'algorithme des k-means.   </p>
<h2 id="-_12">-</h2>
<h2 id="-_13">-</h2>
<h2 id="-_14">-</h2>
<h2 id="-_15">-</h2>
<h2 id="-_16">-</h2>
<h2 id="-_17">-</h2>
<ul>
<li></li>
</ul>
<p><strong>Correction</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># on centre les données  </span>
<span class="n">X_cr</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># graphique en fonction du nombre de classes : </span>
<span class="n">somme_distance</span> <span class="o">=</span> <span class="p">[</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span><span class="o">.</span><span class="n">inertia_</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">somme_distance</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Nombre de classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Distance des points à leur centroïde&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">26</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_20_0.png"></p>
<p>Visuellement on aurait dit 5 classes mais le graphique suggère plutôt 4.  On peut faire les 2 :  </p>
<div class="highlight"><pre><span></span><code><span class="c1"># 5 classes</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>
<span class="n">classes_5</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>

<span class="c1"># 4 classes</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>
<span class="n">classes_4</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>

<span class="c1"># on fait les plots </span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_cr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_cr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">classes_5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;plasma&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Avec 5 classes&quot;</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_cr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_cr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">classes_4</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;plasma&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Avec 4 classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_22_0.png"></p>
<h1 id="dbscan">DBSCAN</h1>
<p>Le clustering avec DBSCAN suit une logique différente de celles des k-means. Cette fois, l'algorithme va parcourir les points un par un et compter le nombre de points voisins en fonction d'une distance epsilon que l'on aura paramétrée. Le point sera alors considéré comme :<br>
- un core point si son voisinage proche (&lt; epsilon) contient au moins k points (k est à paramétrer)<br>
- un border point si son voisinage proche contient moins de k points mais qu'il se trouve dans le voisinage proche d'un point (k différent de 0).<br>
- un bruit (noise) s'il n'est ni un core point ni un border point.  </p>
<p>Les points d'une même classe sont donc tous ceux pouvant être reliés par des core points communs. L'algorithme fonctionne donc de la manière suivante :<br>
- Il s'initialise sur un point, s'il détermine que c'est un core point il continue à déterminer l'ensemble des points de son voisinage jusquèà qu'il tombe sur un point qui n'a pas le minimum requis de voisins (border point ou noise).<br>
- Il passe à un autre point qui n'a pas été visité et continue.  </p>
<p>À noter : Un point peut être considéré comme un bruit dans un premier temps puis redéfini comme un border point si un point visité à son voisingae s'avère être un core point.</p>
<h2 id="differences-avec-k-means">Différences avec k-means</h2>
<p>Ici l'algorithme ne cherche pas à catégoriser les données en un nombre de classes défini mais à mettre en évidence des zones de densité de points. C'est l'algorithme lui-même qui va définir un nombre de classes en fonction du nombre de zones denses qu'il aura parcourues. De plus, certains points peuvent ne pas être classés (les bruits). <br>
Le gros avantage de cet algorithme est donc qu'il ne présuppose pas a priori la forme de la relation entre les points d'une classe mais met en évidence un certain nombre de zones denses.<br>
Là encore une vidéo peut rendre tout ça  plus clair : </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;h53WMIImUuc&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>

<p><iframe
    width="800"
    height="300"
    src="https://www.youtube.com/embed/h53WMIImUuc"
    frameborder="0"
    allowfullscreen
></iframe>
</p>
<h2 id="implementation-sous-python">Implémentation sous python</h2>
<p>La fonction <code>DBSCAN</code> est disponible dans scikit learn : </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
</code></pre></div>

<p>Les deux paramètres à déterminer sont <code>min_sample</code>, le nombre de points minimum à trouver au voisinage d'un point pour qu'il soit considéré comme un core-point, et <code>eps</code> la distance en dessous de laquel on considère qu'un point est au voisinage d'un autre.</p>
<p><strong>Exercice</strong><br>
- Faites un DBSCAN sur les données précédentes sur lesquelles le k-means ne fonctionnait pas correctement.<br>
- Représentez graphiquement les résultats en faisant à chaque fois varier <code>min_sample</code> et <code>eps</code>. Arrivez-vous à trouver un paramétrage qui permette de retrouver les 5 classes que l'on voit visuellement?<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
-<br>
<strong>Correction</strong> : </p>
<div class="highlight"><pre><span></span><code><span class="n">j</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">min_samples</span> <span class="o">=</span> <span class="n">i</span><span class="p">)</span>
<span class="n">predict_db</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">labels_</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([ 0,  1,  2, -1,  1,  0,  1,  3,  1,  4,  0,  4,  2,  4,  2,  2,  3,</span>
<span class="err">        3,  4,  1,  0,  3,  2,  2,  3,  4,  2,  1,  3,  4,  0,  1,  0,  2,</span>
<span class="err">        2,  1,  1,  2,  3,  4,  1,  0,  3,  2,  4,  3,  1,  1,  3,  2,  3,</span>
<span class="err">        2,  3,  1,  0,  1,  4,  0,  4,  1,  1,  4,  4,  0,  1,  4,  4,  4,</span>
<span class="err">        0,  0,  2,  3,  1,  2,  0,  0,  2,  2,  0,  4,  0,  1,  0,  1,  0,</span>
<span class="err">        3,  1,  0,  3,  4,  4,  2,  3,  1,  1,  3,  1,  3,  1,  3,  1,  0,</span>
<span class="err">        3,  3,  1,  2,  4,  2,  1,  3,  4,  3,  3,  3,  3,  3,  2,  0,  3,</span>
<span class="err">        0,  1,  2,  2,  4,  4,  1,  0,  2,  3,  2,  3,  2,  3,  3,  4,  3,</span>
<span class="err">        2,  0,  4,  3,  0,  3,  2,  4,  2,  3,  1,  1,  2,  2,  1,  4,  1,</span>
<span class="err">        4,  2,  3,  2,  4,  0,  1,  4,  3,  0,  3,  1,  2,  2,  3,  0,  2,</span>
<span class="err">        0,  0,  3, -1,  0,  1,  2,  3,  0,  0,  1,  3,  0,  3,  0,  1,  4,</span>
<span class="err">        3,  0,  0,  3,  1,  0,  1,  0,  0,  0,  0,  2,  0,  0,  3,  2,  0,</span>
<span class="err">        3,  3,  4,  4,  3,  4,  4,  2,  2,  3,  4,  1,  0,  4,  3,  4,  3,</span>
<span class="err">        0,  0,  1,  3,  4,  3,  2,  2,  3,  2,  0,  1,  2,  0,  0,  4,  4,</span>
<span class="err">        3,  2,  0,  2,  4,  4,  2,  4,  1,  4,  4,  1,  1,  4,  2,  0,  1,</span>
<span class="err">        1,  0,  0,  1,  1,  4,  0, -1,  1,  4,  2,  0,  3,  2,  0,  4,  2,</span>
<span class="err">        4,  3,  2,  0,  3,  0,  4,  0,  2,  1,  2,  2,  2,  0,  3,  4,  2,</span>
<span class="err">        3,  1,  1,  1,  2,  1,  3,  1,  3,  0,  1,  4,  3,  4,  4,  1,  1,</span>
<span class="err">        4,  1,  1,  4,  4,  4,  3,  1,  1,  0,  0,  4,  1,  0,  1,  2,  1,</span>
<span class="err">        1,  3,  2,  1,  3,  3,  1,  0,  2,  0,  4,  4,  0,  2,  2,  0,  0,</span>
<span class="err">        4,  2,  3,  3,  4,  2,  2,  2,  3,  1,  4,  2,  2,  4,  0,  1,  0,</span>
<span class="err">        4,  4,  2,  1,  1,  4,  0,  3,  2,  1,  1,  4,  0,  2,  1,  1,  1,</span>
<span class="err">        2,  0,  1,  3,  2,  1,  2,  1,  4,  0,  3,  4,  3,  1,  2,  2,  3,</span>
<span class="err">        0,  2,  3,  3,  4,  0,  3,  1,  1,  0,  4,  0,  3,  0,  3,  4,  3,</span>
<span class="err">        2,  3,  3,  4,  1,  4,  0,  4,  4,  0,  3,  4,  1,  1,  4,  3,  4,</span>
<span class="err">        2,  2,  0,  1,  3, -1,  4,  2,  0,  2,  3,  0,  2,  1,  0,  0,  3,</span>
<span class="err">        2,  0,  4,  4,  1,  4,  4,  0,  4,  2,  1,  4,  3,  2,  2,  0,  3,</span>
<span class="err">        2,  0,  2,  2,  4,  1,  3,  4,  3,  4,  0,  2,  2,  2,  0,  4,  3,</span>
<span class="err">        3,  3,  1,  1,  3,  3,  3,  4,  4,  3,  4,  3,  2,  1,  0,  0,  2,</span>
<span class="err">        0,  4,  0,  1,  1,  3,  2,  0,  4,  0,  2,  0,  3,  4,  0,  1,  1,</span>
<span class="err">        2,  1,  1,  3,  2,  1,  2,  0,  4,  4,  1,  2,  2,  3,  1,  1,  1,</span>
<span class="err">        2,  2,  4,  0,  2,  2,  0,  1,  2,  4,  3,  0,  0,  3,  3,  0,  0,</span>
<span class="err">        3,  3,  3,  1,  0,  0,  2,  0,  0,  1,  1,  2,  4,  3,  3,  2,  4,</span>
<span class="err">        3,  2,  2,  0,  3,  4,  4,  0,  1,  4,  1,  2,  0,  4,  0,  4,  4,</span>
<span class="err">        3,  4,  2,  1,  3,  1,  3,  0,  1,  4,  4,  4,  4,  1,  4,  0,  3,</span>
<span class="err">        4,  2,  4,  2,  1], dtype=int64)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># on initialise des listes vides où mettre nos résultats</span>
<span class="n">predict_DB</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">list_eps_minsamp</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># On fait DBSCAN pour les différentes combinaisons de eps et min_samples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">min_samples</span> <span class="o">=</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_cr</span><span class="p">)</span>
        <span class="n">predict_DB</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
        <span class="n">list_eps_minsamp</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>


<span class="c1"># on représente graphiquement le résultat</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predict_DB</span><span class="p">)):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">predict_DB</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;plasma&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Min sample et epsilon :  = </span><span class="si">{</span><span class="n">list_eps_minsamp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="Pelican" src="../images/output_33_0.png"></p>
<p><strong>Attention</strong> : Pour min_sample = 20, les résultats à epsilon = 0.05 et epsilon à 0.5 n'ont pas du tout le même sens : </p>
<div class="highlight"><pre><span></span><code><span class="n">predict_DB</span><span class="p">[</span><span class="mi">6</span><span class="p">][</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,</span>
<span class="err">       -1, -1, -1], dtype=int64)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">predict_DB</span><span class="p">[</span><span class="mi">8</span><span class="p">][</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="err">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)</span>
</code></pre></div>

<h2 id="sources">Sources</h2>
<p>Un article très intéressant sur DBSCAN et k-means sur lequel je m'appuie pour certaines exemples : https://towardsdatascience.com/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea</p>
<h2 id="cas-pratique-1-classification-des-fromages">Cas pratique 1 : classification des fromages</h2>
<ul>
<li>Importez la base carac_fromages.txt et affichez quelques stats descriptives sur ses variables   </li>
</ul>
<p>On cherche à voir si certains fromages sont plus proches que d'autres en fonction de leurs caractéristiques.  </p>
<ul>
<li>Transformez les variables numériques de manière à pouvoir mener une analyse en k-means.  </li>
<li>Justifiez un choix de nombre de classes avec cette méthode et opérez le clustering de ces données.  </li>
<li>Observez les noms des fromages d'une même classe. Trouvez-vous logique qu'ils soient associés?  </li>
<li>Faites une classification avec DBSCAN et comparez les résultats. </li>
</ul>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=K-means et DBSCAN&amp;url=https://statoscop.github.io/blog/kmeans-dbscan.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://statoscop.github.io/blog/kmeans-dbscan.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=https://statoscop.github.io/blog/kmeans-dbscan.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="https://statoscop.github.io/blog/tag/python.html">python</a><a href="https://statoscop.github.io/blog/tag/scikit-learn.html">scikit-learn</a><a href="https://statoscop.github.io/blog/tag/clustering.html">clustering</a>                </aside>

                <div class="clear"></div>


                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Blog réalisé par les fondateurs de Statoscop et publié avec
            <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a>
          </span>
          <span class="credits-software">
            Thème <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a>
          </span>
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="https://statoscop.github.io/blog/theme/js/script.js"></script>

</body>
</html>