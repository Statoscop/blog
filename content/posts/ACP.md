Title: Analyse en composantes principales avec Python
Author: Antoine
Date: '2021-04-16'
Slug: acp-python
Category: Python, Stats & ML
Tags: Python, Machine Learning, Statistiques, Data Science  
Cover: images/cover_2.png
Summary: Pr√©sentation et exemples d'utilisation de l'ACP en statistiques et data science.

[TOC]

Dans cet article, nous allons essayer de comprendre intuitivement comment fonctionne l'analyse en composantes principales. Nous pr√©senterons ensuite √† quoi celle-ci peut servir en prenant les exemples d'une analyse exploratoire des donn√©es et d'une probl√©matique de r√©duction de dimension.   

# Explication introductive

L'analyse en composantes principales est une m√©thode consistant √† transformer des variables corr√©l√©es entre elles en nouvelles variables. Chacune de ces nouvelles variables est le r√©sultat d'une combinaison lin√©aire des anciennes variables. 

*Note : une combinaison lin√©aire de 3 variables $V_1$, $V_2$ et $V_3$ s'√©crit $\alpha_1.V_1 + \alpha_2.V_2 + \alpha_3.V_3$ o√π les $\alpha_i$ sont des coefficients r√©els.*

Ces nouvelles variables sont appel√©es __composantes principales__ et sont, par contruction, d√©corr√©l√©es les unes des autres.  

Autrement dit, l'ACP projette vos donn√©es dans un nouvel espace. La premi√®re composante principale est construite de mani√®re √† capter la plus grande variance possible de vos donn√©es, la seconde la part la plus importante de la variance possible __restant √† expliquer__, et ainsi de suite.  

Une illustration brillante de ce processus est propos√©e par <a href="https://www.allisonhorst.com/" target="_blank">Allison Horst</a>. Elle <a href="https://twitter.com/allison_horst/status/1288904459490213888" target="_blank">repr√©sente</a> un jeu de donn√©es √† deux dimensions avec des crevettes et l'analyse en composantes principales comme les passages d'un requin-baleine affam√© :  


![Pelican](../images/acp/output_3_0.png)


![Pelican](../images/acp/output_4_0.png)



La probl√©matique du requin-baleine est en effet la m√™me que celle de la cr√©ation d'une premi√®re composante principale : quel axe choisir pour avaler un maximum de crevettes d√®s le premier passage? L'axe choisi va ressembler √† celui-ci :  



![Pelican](../images/acp/output_6_0.png)



Il s'agit pour le requin de choisir la droite de sorte qu'il y ait un maximum de crevettes sur son parcours ce qui revient √† ce que les crevettes soient le plus proche possible de cette droite. Math√©matiquement, la premi√®re composante principale est la combinaison lin√©aire des deux axes $x$ et $y$ qui maximise l'inertie projet√©e ce qui revient √† minimiser les √©carts entre les points et cette droite.

Dans cet exemple, la seconde composante principale sera l'axe perpendiculaire √† ce premier axe.

Si les points √©taient parfaitement align√©s sur une ligne, l'ensemble de la variance serait expliqu√© par la premi√®re composante et on serait parvenus √† r√©duire le nombre de dimensions de notre probl√®me sans perte d'information.


> üëã Nous c'est Antoine et Louis de Statoscop, une coop√©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>    


# Mise en oeuvre d'une ACP
D'accord, on a projet√© notre jeu de donn√©es dans un nouvel espace avec des nouvelles "variables" d√©crites comme combinaisons lin√©aires des pr√©c√©dentes telles que la premi√®re explique la plus grande partie de la variance possible, la seconde la plus grande partie de la variance restant √† expliquer, etc... Mais √ßa nous sert √† quoi?   

## Analyse exploratoire de nos donn√©es  
La caract√©ristique des composantes principales par rapport au jeu de donn√©es non transform√© est que les premi√®res composantes principales ont un fort pouvoir discriminant, puisqu'elles expliquent une grande partie de la variance totale du jeu de donn√©es. Ainsi, repr√©senter notre jeu de donn√©es par rapport aux deux premiers axes de l'ACP peut permettre de v√©rifier que ces donn√©es permettent bien de distinguer diff√©rentes classes.  

Prenons comme exemple la base de donn√©es `wine` que l'on peut charger directement depuis le module `sklearn`. Cette base de donn√©es contient des r√©sultats d'analyses chimiques de 178 vins de 3 diff√©rents producteurs. Ces r√©sultats sont synth√©tis√©s par 13 mesures diff√©rentes que l'on retrouve dans les donn√©es. Pour voir si ces mesures permettent ou non de distinguer les vins des trois producteurs, nous allons commencer par repr√©senter les vins sur l'espace des deux premi√®res composantes principales. Pour cela, on importe les donn√©es et on les centre-r√©duit avant d'appliquer notre ACP avec la fonction `sklearn.decomposition.PCA`. On param√®tre celle-ci pour qu'elle nous renvoie seulement les deux premi√®res composantes :  


```python
import numpy as np
# Import fonction ACP
from sklearn.decomposition import PCA

# Import donn√©es
from sklearn.datasets import load_wine

values, target = load_wine(return_X_y=True)
target_names = load_wine().target_names
feature_names = load_wine().feature_names
# on standardise nos donn√©es : 
from sklearn.preprocessing import StandardScaler
values_cr = StandardScaler().fit_transform(values)

# On param√®tre notre PCA pour garder les deux premi√®res composantes
pca=PCA(2)
pca_wine = pca.fit_transform(values_cr)

# en sortie : le m√™me nombre de lignes que les donn√©es en entr√©es
# et le nombre de variables correspondant au nombre de composantes
# conserv√©es
pca_wine.shape
```




    (178, 2)



En sortie, nous obtenons les vecteurs des deux premi√®res composantes dans l'objet `pca_wine`. Notons que nous aurions pu param√©trer la fonction `PCA` de mani√®re √† ce qu'elle nous renvoie le nombre de composantes n√©cessaire √† expliquer `X`% de la variance, comme nous le ferons par la suite. Depuis l'objet `pca`, on peut voir le vecteur de la variance expliqu√©e par chaque composante avec `pca.explained_variance_ratio_` et donc la variance totale expliqu√©e par nos deux composantes en sommant les √©l√©ments de ce vecteur :  


```python
pca.explained_variance_ratio_.sum()
```




    0.554063383569353



On explique donc 55 % de la variance totale de nos donn√©es avec 2 composantes, alors que celle-ci contient 13 variables. Voyons si cela suffit √† discriminer nos 3 producteurs visuellement :  

<!--- ![Pelican](../images/acp/output_13_0.png)-->
<img alt="Pelican" src="../images/acp/output_13_0.png" style="max-width:80% !important" >

On constate ici que les 3 producteurs sont bien r√©partis dans des zones distinctes du plan et ce r√©sultat semble montrer que chacun produit des types de vin caract√©ristiques.

On peut se convaincre que l'ACP a bien jou√© son r√¥le en produisant le m√™me type de sch√©ma avec deux autres variables originales du jeu de donn√©es (sans transformation lin√©aire), disons le degr√© d'alcool et l'intensit√© de la couleur. On s'attend bien s√ªr √† ce que les classes soient moins discrimin√©es qu'avec les deux premi√®res composantes principales :  

<img alt="Pelican" src="../images/acp/output_15_0.png" style="max-width:80% !important" >

Ces variables permettent de distinguer des tendances, comme le fait que le producteur 1 produit des vins plut√¥t moins alcoolis√©s et dont la couleur est peu intense alors que le producteur 0 produit des vins plus alcoolis√©s. Mais ces variables seules ne permettent pas de partitionner nos classes aussi clairement qu'avec les deux premi√®res composantes de notre ACP.   

L'ACP ne permet certes pas au premier coup d'oeil de proposer une interpr√©tation des r√©sultats, mais il est n√©anmoins possible d'√©tudier comment chaque variable contribue aux composantes avec l'instruction `pca.components_` :  

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Noms des variables</th>
      <th>Composante 1</th>
      <th>Composante 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>alcohol</td>
      <td>0.144329</td>
      <td>-0.483652</td>
    </tr>
    <tr>
      <th>1</th>
      <td>malic_acid</td>
      <td>-0.245188</td>
      <td>-0.224931</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ash</td>
      <td>-0.002051</td>
      <td>-0.316069</td>
    </tr>
    <tr>
      <th>3</th>
      <td>alcalinity_of_ash</td>
      <td>-0.239320</td>
      <td>0.010591</td>
    </tr>
    <tr>
      <th>4</th>
      <td>magnesium</td>
      <td>0.141992</td>
      <td>-0.299634</td>
    </tr>
    <tr>
      <th>5</th>
      <td>total_phenols</td>
      <td>0.394661</td>
      <td>-0.065040</td>
    </tr>
    <tr>
      <th>6</th>
      <td>flavanoids</td>
      <td>0.422934</td>
      <td>0.003360</td>
    </tr>
    <tr>
      <th>7</th>
      <td>nonflavanoid_phenols</td>
      <td>-0.298533</td>
      <td>-0.028779</td>
    </tr>
    <tr>
      <th>8</th>
      <td>proanthocyanins</td>
      <td>0.313429</td>
      <td>-0.039302</td>
    </tr>
    <tr>
      <th>9</th>
      <td>color_intensity</td>
      <td>-0.088617</td>
      <td>-0.529996</td>
    </tr>
    <tr>
      <th>10</th>
      <td>hue</td>
      <td>0.296715</td>
      <td>0.279235</td>
    </tr>
    <tr>
      <th>11</th>
      <td>od280/od315_of_diluted_wines</td>
      <td>0.376167</td>
      <td>0.164496</td>
    </tr>
    <tr>
      <th>12</th>
      <td>proline</td>
      <td>0.286752</td>
      <td>-0.364903</td>
    </tr>
  </tbody>
</table>
</div>



Ce tableau repr√©sente les coefficients de la combinaison lin√©aire des variables pour chaque composante. Il nous permet par exemple de constater que l'intensit√© de la couleur et l'alcool jouent fortement et n√©gativement sur la seconde composante. Cela correspond √† ce que l'on observait dans les deux graphiques pr√©c√©dents puisque les vins des producteurs 0 et 2 ont des valeurs n√©gatives sur l'axe de la seconde composante (1er graphique) et ce sont bien ceux dont le taux en alcool et l'intensit√© de la couleur sont les plus importants (2e graphique)

## Utilisation de l'ACP pour la r√©duction de dimensions  
La propri√©t√© de l'ACP de capter une partie importante de la variance des donn√©es √† partir de moins de variables est particuli√®rement int√©ressante dans le domaine du Machine Learning pour √™tre capable de fournir des pr√©dictions avec des mod√®les plus l√©gers (car utilisant moins de variables) et des r√©sultats au moins aussi performants.  
Pour notre exemple, m√™me si la r√©duction de dimensions n'est pas un enjeu fondamental vu le faible nombre de variables, nous pouvons tester si nous parvenons √† faire un mod√®le de pr√©diction de l'origine du vin (producteur 0, 1 ou 2) en r√©duisant le nombre de dimensions.  
Tout d'abord, commen√ßons par d√©terminer ce nombre de dimensions. Le graphique suivant nous donne l'√©volution de la variance expliqu√©e en fonction du nombre de composantes :   

<img alt="Pelican" src="../images/acp/output_20_0.png" style="max-width:80% !important" >


L'ACP permettrait d'expliquer plus de 70% de la variance totale d√®s 4 composantes. Pour voir si cela est suffisant pour entra√Æner un mod√®le de pr√©diction, on peut comparer les performances d'un arbre de classification sur les donn√©es transform√©es apr√®s PCA et sur les donn√©es brutes. On utilise une m√©thode de validation crois√©e pour estimer les performances du mod√®le qui consiste √† partitionner les donn√©es en 5 groupes et √† entra√Æner les donn√©es sur 4 groupes et les tester sur celui restant. On fait cela 5 fois pour parcourir le champ des possibles et on √©value la pr√©cision globale du mod√®le en faisant la moyenne de ces 5 r√©sultats. Cette m√©thode doit permettre d'estimer la qualit√© du mod√®le sur des donn√©es sur lesquelles il n'a pas √©t√© entra√Æn√© et de ne pas prendre en compte le surapprentissage dans son √©valuation. Le tableau suivant donne les taux de pr√©cision obtenus pour chaque m√©thode, c'est √† dire le nombre de vins correctement classifi√©s sur le nombre de vins total.


```python
# Plut√¥t que de renseigner le nombre de composantes 
# on renseigne la valeur minimum de la variance 
# expliqu√©e totale que l'on souhaite
pca=PCA(0.70) 
wine_pca = pca.fit_transform(values_cr)

# On entra√Æne notre mod√®le et on l'√©value avec une 
# m√©thode de validation crois√©e 
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0)
mean_pca = cross_val_score(clf, wine_pca, target, cv=5).mean()
mean_all = cross_val_score(clf, values, target, cv=5).mean()

pd.DataFrame({"Pr√©cision moyenne apr√®s ACP" : [mean_pca], 
              "Pr√©cision moyenne sans ACP" : [mean_all], 
              "Nombre de composantes" : [wine_pca.shape[1]]})
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pr√©cision moyenne apr√®s ACP</th>
      <th>Pr√©cision moyenne sans ACP</th>
      <th>Nombre de composantes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.933175</td>
      <td>0.887619</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>



On constate que l'ACP n'a pas seulement permis de r√©duire le nombre de dimensions de notre probl√®me, elle nous offre aussi une pr√©cision globale du mod√®le sup√©rieure. Cela n'est pas toujours le cas - puisque √ßa d√©pend de votre probl√©matique, des variables explicatives dont vous disposez et du nombre de composantes que vous retenez - mais ici c'est d√ª au fait qu'elle permet de r√©duire le bruit associ√© aux donn√©es en ne conservant qu'une partie de l'information totale. Cela permet ainsi de pr√©venir les probl√®mes de surapprentissage, c'est √† dire le fait que le mod√®le explique parfaitement les donn√©es d'entra√Ænement mais se g√©n√©ralise mal √† de nouvelles donn√©es. Ce sujet est abord√© dans [cet article de notre blog sur l'arbitrage biais/variance](https://blog.statoscop.fr/larbitrage-biaisvariance-dans-la-modelisation-de-donnees.html).  

C'est tout pour aujourd'hui! Si vous voulez voir d'autres exemples d'utilisation de l'ACP, je vous conseille <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" target="_blank">cet article</a> qui aborde notamment le cas du traitement des images, pour lequel il est particuli√®rement int√©ressant de r√©duire le nombre de dimensions. N'h√©sitez pas √† [visiter notre site](https://www.statoscop.fr) et √† nous suivre sur [Twitter](https://twitter.com/stato_scop) pour ne pas rater les prochains articles! Vous pouvez trouver le notebook avec l'ensemble du code ayant servi √† g√©n√©rer cette note sur le <a href="https://github.com/Statoscop/notebooks-blog" target="_blank">github de Statoscop</a>.   
  

<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  