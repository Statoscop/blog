Title: RÃ©seaux neuronaux convolutifs avec R et Keras
Author: Antoine
Date: '2025-02-18'
Category: R, Deep learning
Tags: R, Keras, CNN, Machine Learning, Data Science
Cover: images/cover_22.png
twitter_image: images/cover_22.png
Summary: DÃ©monstration de l'utilisation du package `keras3` pour faire du Deep learning avec R.

[TOC]

On le sait, le **dÃ©bat entre R et Python** est acharnÃ© et souvent peu productif. En Machine Learning, tout le monde ou presque s'accorde Ã  dire que Python a une longueur d'avance, surtout pour les modÃ¨les de Deep Learning (rÃ©seaux de neurones).  

Heureusement, il existe souvent des alternatives pour celles et ceux ne souhaitant pas se mettre Ã  un autre langage (mÃªme si on vous y encourage!). Ainsi **le package R `keras3` permet d'accÃ©der Ã  la cÃ©lÃ¨bre API Python** du mÃªme nom. Nous vous montrons donc dans cet article comment elle fonctionne.   

# PrÃ©sentation de la librairie Keras

## Origine de la librairie

Keras est une **API Python qui permet de crÃ©er et entraÃ®ner des modÃ¨les de Deep Learning**. Elle a l'avantage d'offrir une interface simple et une syntaxe claire. Elle permet Ã©galement d'accÃ©der Ã  de nombreux modÃ¨les prÃ©-entraÃ®nÃ©s qui **peuvent ensuite facilement Ãªtre _fine-tunÃ©s_ sur d'autres donnÃ©es**. Elle a Ã©tÃ© rÃ©cemment intÃ©grÃ©e Ã  `TensorFlow` mais peut s'appuyer sur d'autres backends.

Keras dispose aussi d'une [documentation trÃ¨s complÃ¨te](https://keras.io/api/), et sa popularitÃ© permet de retrouver de nombreux exemples d'utilisation en ligne. Elle est le plus souvent utilisÃ©e grÃ¢ce au package Python `keras` ou via TensorFlow avec `tf.keras`, qui permettent d'exploiter facilement ses diffÃ©rentes mÃ©thodes.   

## Installation du package sur R  

GrÃ¢ce notamment Ã  la puissance de `reticulate`, un package permettant de faire tourner Python depuis une session R, on dispose maintenant d'un package R `keras`, que nous vous prÃ©sentons ici. Nous utiliserons la derniÃ¨re version disponible, le package `keras3`. La premiÃ¨re Ã©tape est bien sur d'installer le package avec `install.packages("keras3")` puis de lancer la fonction `keras3::install_keras()` qui permet d'installer Python et un environnement anaconda dÃ©diÃ©.


```r
library(keras3)
install_keras()
```

Si vous avez du GPU, vous pouvez vÃ©rifier que **votre device a bien Ã©tÃ© repÃ©rÃ©** avec l'instruction suivante (normalement, `tensorflow` a Ã©tÃ© installÃ© au moment de l'installation de `keras3`) :    


```r
tensorflow::tf$config$list_physical_devices("GPU")
```

```
## [[1]]
## PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
```

> ğŸ‘‹ Nous c'est Antoine et Louis de Statoscop, une coopÃ©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>      


# EntraÃ®ner un premier rÃ©seau convolutionnel

Pour notre petite dÃ©monstration, nous allons utiliser la cÃ©lÃ©brissime base de donnÃ©es `mnist` qui contient **des images de chiffres Ã©crits Ã  la main**. Le but est donc de dÃ©velopper un modÃ¨le capable de reconnaÃ®tre les chiffres manuscrits.  

## PrÃ©-traiter vos donnÃ©es

Les donnÃ©es fournies contiennent 60 000 observations pour les donnÃ©es d'entraÃ®nements et 10 000 pour les donnÃ©es de test. Les images Ã©tant renseignÃ©es directement sous forme de matrice dans le package `keras3` nous n'avons besoin que de normaliser ces matrices sur [0, 1]. On passe aussi la variable d'intÃ©rÃªt, qui donne la valeur du chiffre en question, sous un *one-hot-encoder* avec la fonction `to_categorical`. On obtient ainsi un vecteur de taille 10 contenant un indicateur pour chaque chiffre. 


```r
library(keras3)

mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test<- mnist$test$x
y_test <- mnist$test$y

# on normalise sur [0, 1]
x_train <- x_train/255
x_test <- x_test/255

# one-hot-encoder pour la variable d'intÃ©rÃªt avec to_categorical
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)
```

## Initialiser et dÃ©finir votre rÃ©seau de neurones profond  

On **initialise le modÃ¨le** avec `keras_model_sequential`, dans lequel on prÃ©cise d'emblÃ©e la dimension des donnÃ©es que le modÃ¨le prendra en entrÃ©e. Puis on ajoute les **diffÃ©rentes couches de neurones**.  

Comme nous sommes dans un rÃ©seau convolutionnel, on commence directement par une couche de convolution, qui permet de faire **ressortir les caractÃ©ristiques de chaque image**. Cela fonctionne grÃ¢ce Ã  un systÃ¨me de filtres, rÃ©alisÃ© en faisant le produit de la matrice de pixels et d'une matrice plus petite, appelÃ©e `feature detector`. On dÃ©finit dans cette Ã©tape le nombre de filtres que l'on souhaite faire passer avec le paramÃ¨tre `filters` et la taille de la matrice `feature detector` avec le paramÃ¨tre `kernel_size`.   

On **applique ensuite aux matrices obtenues une couche de _max pooling_** qui permet de rÃ©duire les dimensions du problÃ¨me en ne conservant que les valeurs les plus importantes. Cette Ã©tape permet aussi de rendre plus souvent comparables **des images ayant des caractÃ©ristiques semblables Ã  des endroits diffÃ©rents**. Le paramÃ¨tre `pool_size` permet de choisir la **taille du sous-ensemble sur lequel on ne va garder que la valeur maximale**. 

On peut ensuite refaire des nouvelles couches de convolution suivies de pooling, en **fonction de la complexitÃ© et de la taille de nos matrices de pixel en entrÃ©e du modÃ¨le**. On termine ces Ã©tapes par une couche `flatten()` qui permet d'obtenir un input en deux dimensions sur lequel on va pouvoir construire notre rÃ©seau de neurones profond avec des couches `dense()`. On choisit ici une couche avec 128 neurones, suivie d'une couche avec 64 neurones. La couche de sortie contient **10 neurones correspondant aux 10 classes possibles**.

Le package R `keras3` permet une syntaxe **trÃ¨s proche de celle que l'on utiliserait sur Python**. On peut en plus relier ces couches avec l'opÃ©rateur `|>`, rendant le code encore plus lisible et aÃ©rÃ©.  Ã€ noter qu'une fois que le modÃ¨le `my_first_r_cnn` est crÃ©Ã©, **il n'est pas nÃ©cessaire de le rÃ©assigner Ã  chaque ajout de layers**. L'opÃ©rateur `|>` met Ã  jour directement l'objet `my_first_r_cnn`.


```r
# Initialisation du modÃ¨le  
my_first_r_cnn <- keras_model_sequential(input_shape = c(28,28,1))

# convolution et max pooling
my_first_r_cnn |> 
  # on dÃ©finit les dimension des inputs dans une couche dÃ©diÃ©e
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                # l'activation relu "casse" une linÃ©aritÃ© qui a pu
                # Ãªtre introduite avec la convolution
                activation = 'relu') |> 
  layer_max_pooling_2d(pool_size = c(2, 2)) |> 
  layer_conv_2d(filters = 16, kernel_size = c(2, 2), 
                activation = 'relu') |> 
  layer_max_pooling_2d(pool_size = c(2, 2)) |> 
  layer_flatten() |> 
  layer_dense(units = 128, activation = 'relu') |> 
  layer_dense(units = 64, activation = 'relu') |> 
  layer_dense(units = 10, activation = 'softmax')
```

On peut illustrer la **structure de notre modÃ¨le** avec la fonction `summary()` : 


```r
summary(my_first_r_cnn)
```

```
## Model: "sequential"
## â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
## â”ƒ Layer (type)                      â”ƒ Output Shape             â”ƒ       Param # â”ƒ
## â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
## â”‚ conv2d (Conv2D)                   â”‚ (None, 26, 26, 32)       â”‚           320 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ max_pooling2d (MaxPooling2D)      â”‚ (None, 13, 13, 32)       â”‚             0 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ conv2d_1 (Conv2D)                 â”‚ (None, 12, 12, 16)       â”‚         2,064 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ max_pooling2d_1 (MaxPooling2D)    â”‚ (None, 6, 6, 16)         â”‚             0 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ flatten (Flatten)                 â”‚ (None, 576)              â”‚             0 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense (Dense)                     â”‚ (None, 128)              â”‚        73,856 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_1 (Dense)                   â”‚ (None, 64)               â”‚         8,256 â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_2 (Dense)                   â”‚ (None, 10)               â”‚           650 â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
##  Total params: 85,146 (332.60 KB)
##  Trainable params: 85,146 (332.60 KB)
##  Non-trainable params: 0 (0.00 B)
```


## EntraÃ®ner et Ã©valuer les performances de votre modÃ¨le

La premiÃ¨re Ã©tape est de compiler l'objet crÃ©Ã© prÃ©cÃ©demment, **en dÃ©finissant l'_optimizer_ qui sera utilisÃ©**, la fonction de perte et la mÃ©trique sur laquelle on souhaite optimiser le rÃ©sultat du modÃ¨le :  
  


```r
compile(my_first_r_cnn,
        optimizer = 'adam',
        loss = 'categorical_crossentropy',
        metrics = 'accuracy') 
```

Ensuite, on peut **lancer l'entraÃ®nement avec une instruction `fit()`** que l'on applique Ã  nos donnÃ©es d'entraÃ®nement. On y dÃ©finit notamment le paramÃ¨tre `batch_size` qui permet de dÃ©terminer combien d'images le modÃ¨le va traiter avant d'ajuster ses coefficients. Le paramÃ¨tre `epochs` dÃ©finit le nombre de fois oÃ¹ le modÃ¨le passe sur toutes les donnÃ©es. Enfin, on split nos donnÃ©es d'entraÃ®nement dont on conserve 30% comme donnÃ©es de validation afin de d'**Ã©valuer correctement les performances de notre modÃ¨le** :  


```r
history <- my_first_r_cnn %>% fit(
  x_train, y_train,
  epochs = 30,
  batch_size = 64,
  validation_split = 0.3)
```

```
## Epoch 1/30
## 657/657 - 5s - 8ms/step - accuracy: 0.9120 - loss: 0.2766 - val_accuracy: 0.9623 - val_loss: 0.1200
## Epoch 2/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9766 - loss: 0.0753 - val_accuracy: 0.9774 - val_loss: 0.0733
## Epoch 3/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9849 - loss: 0.0507 - val_accuracy: 0.9799 - val_loss: 0.0608
## Epoch 4/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9878 - loss: 0.0395 - val_accuracy: 0.9832 - val_loss: 0.0553
## Epoch 5/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9895 - loss: 0.0316 - val_accuracy: 0.9859 - val_loss: 0.0490
## Epoch 6/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9923 - loss: 0.0234 - val_accuracy: 0.9826 - val_loss: 0.0640
## Epoch 7/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9929 - loss: 0.0207 - val_accuracy: 0.9855 - val_loss: 0.0533
## Epoch 8/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9946 - loss: 0.0165 - val_accuracy: 0.9866 - val_loss: 0.0507
## Epoch 9/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9941 - loss: 0.0165 - val_accuracy: 0.9835 - val_loss: 0.0632
## Epoch 10/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9870 - val_loss: 0.0501
## Epoch 11/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9958 - loss: 0.0123 - val_accuracy: 0.9853 - val_loss: 0.0613
## Epoch 12/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9965 - loss: 0.0103 - val_accuracy: 0.9872 - val_loss: 0.0570
## Epoch 13/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9871 - val_loss: 0.0544
## Epoch 14/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9970 - loss: 0.0082 - val_accuracy: 0.9871 - val_loss: 0.0583
## Epoch 15/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9972 - loss: 0.0091 - val_accuracy: 0.9867 - val_loss: 0.0624
## Epoch 16/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 0.9871 - val_loss: 0.0623
## Epoch 17/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9976 - loss: 0.0070 - val_accuracy: 0.9857 - val_loss: 0.0693
## Epoch 18/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9970 - loss: 0.0096 - val_accuracy: 0.9853 - val_loss: 0.0693
## Epoch 19/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9983 - loss: 0.0051 - val_accuracy: 0.9862 - val_loss: 0.0690
## Epoch 20/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 0.9882 - val_loss: 0.0612
## Epoch 21/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 0.9881 - val_loss: 0.0596
## Epoch 22/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 0.9789 - val_loss: 0.1290
## Epoch 23/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9976 - loss: 0.0074 - val_accuracy: 0.9876 - val_loss: 0.0682
## Epoch 24/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9879 - val_loss: 0.0690
## Epoch 25/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9843 - val_loss: 0.0981
## Epoch 26/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 0.9863 - val_loss: 0.0723
## Epoch 27/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 0.9884 - val_loss: 0.0659
## Epoch 28/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9879 - val_loss: 0.0725
## Epoch 29/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9997 - loss: 9.4193e-04 - val_accuracy: 0.9889 - val_loss: 0.0674
## Epoch 30/30
## 657/657 - 1s - 2ms/step - accuracy: 0.9971 - loss: 0.0086 - val_accuracy: 0.9856 - val_loss: 0.0672
```

On peut illustrer l'Ã©volution de la qualitÃ© du modÃ¨le au fur et Ã  mesure des epochs avec `plot` :  


```r
plot(history)
```

![Pelican](../images/cnn_r_keras/unnamed-chunk-8-1.png)<!-- -->

Enfin, on sort la **performance du modÃ¨le sur nos donnÃ©es test** avec `evaluate`:  


```r
accuracy_test <- my_first_r_cnn |> evaluate(x_test, y_test)
```

```
## 313/313 - 1s - 3ms/step - accuracy: 0.9898 - loss: 0.0522
```

```r
print(accuracy_test)
```

```
## $accuracy
## [1] 0.9898
## 
## $loss
## [1] 0.05221168
```

> Inutile bien sÃ»r de prÃ©ciser que nous sommes dans un cas de classification relativement simple et avec Ã©normÃ©ment de donnÃ©es de bonne qualitÃ©, donc ne cherchez pas Ã  atteindre de tels niveaux de prÃ©cision sur des donnÃ©es un peu plus compliquÃ©es! 

# PrÃ©dictions Ã  partir du modÃ¨le prÃ©-entraÃ®nÃ©  

Prenons par exemple la 20e image de l'Ã©chantillon test : 


```r
index <- 20 # On prend la 20e image"
image_data <- x_test[index,,]  # SÃ©lection de l'image
label <- mnist$test$y[index]  # Label associÃ©

# Inverser les couleurs pour un affichage correct
image_data <- t(apply(image_data, 2, rev))

# Afficher l'image en niveaux de gris
image(image_data, col = gray((0:255)/255), 
      main = paste("Label :", label),
      axes = FALSE, asp = 1)
```

![Pelican](../images/cnn_r_keras/unnamed-chunk-10-1.png)<!-- -->

On peut sortir les __prÃ©dictions des probabilitÃ©s associÃ©es Ã  chaque classe avec la fonction `predict()`__. La prÃ©diction associÃ©e Ã  l'image prÃ©cÃ©dente donne le rÃ©sultat suivant :  


```r
# On prÃ©dit l'ensemble des X_test
matrice_pred <- my_first_r_cnn |> predict(x_test)
```

```
## 313/313 - 1s - 2ms/step
```

```r
# On sort l'index max - 1 (qui correspond Ã  la classe) de l'observation nÂ°20
pred_obs <- which.max(matrice_pred[20,]) - 1

image(image_data, col = gray((0:255)/255), 
      main = paste("Label :", label, "PrÃ©diction :", pred_obs),
      axes = FALSE, asp = 1)
```

![Pelican](../images/cnn_r_keras/unnamed-chunk-11-1.png)<!-- -->


# Deep Learning avec R ou Python?  

Le package `keras3` est **trÃ¨s complet et semble offrir autant de possibilitÃ©s que le module Python**. Il dispose aussi d'une [documentation complÃ¨te et trÃ¨s dÃ©taillÃ©e](https://keras3.posit.co). Cependant, il faut garder en tÃªte qu'il fait tourner Python en arriÃ¨re-plan.   

Le gros avantage pour un utilisateur non aguerri de Python est qu'il va **gÃ©rer pour vous l'environnement anaconda et les dÃ©pendances**, qui peuvent Ãªtre un vrai casse-tÃªte quand on se met Ã  Python. Enfin, la **syntaxe trÃ¨s proche de celle de Python peut vous permettre de commencer Ã  vous accoutumer en vue d'un futur changement de langage**, tout en restant sur votre langage favori en attendant!  

Enfin, on peut imaginer que l'utilisation d'un package R peut Ãªtre trÃ¨s pratique dans le cadre de la mise en oeuvre de mÃ©thodes de statistiques infÃ©rentielles sur des prÃ©dictions de modÃ¨les de Machine Learning, comme par exemple dans [ce papier trÃ¨s intÃ©ressant](https://www.science.org/doi/10.1126/science.adi6000).

C'est la fin de cet article! N'hÃ©sitez pas Ã  [visiter notre site](https://www.statoscop.fr) et Ã  nous suivre sur [BlueSky](https://bsky.app/profile/statoscop.fr) et [Linkedin](https://www.linkedin.com/company/statoscop). Pour retrouver l'ensemble du code ayant servi Ã  gÃ©nÃ©rer cette note, vous pouvez vous rendre sur le [github de Statoscop](https://github.com/Statoscop/notebooks-blog).  

<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  