Title: Le dilemme biais variance dans la mod√©lisation de donn√©es   
Author: Antoine
Date: '2021-11-08'
Category: R, Stats & ML
Tags: R, Rstats, data science, statistiques, Machine Learning 
Cover: images/cover_8.png
twitter_image: images/cover_8.png
Summary: Pr√©sentation des enjeux th√©oriques et pratiques de l'arbitrage biais variance dans la construction d'un mod√®le de pr√©diction.

[TOC]  

L'arbitrage biais variance est souvent √©voqu√© pour caract√©riser les enjeux de la construction d'un mod√®le de pr√©diction performant. L'id√©e de cet article est d'essayer de donner au lecteur les outils th√©oriques de cette question en essayant de privil√©gier une approche intuitive et pratique du probl√®me. Apr√®s avoir d√©fini ce que sont le biais et la variance, on pr√©sente les enjeux de cet arbitrage puis l'application concr√®te dans le cas de l'entra√Ænement d'un mod√®le de Machine Learning.

# Que sont le biais et la variance?

Pour expliquer le plus simplement possible ces concepts, on se place dans le contexte de l'observation de deux variables `Y` et `X`. Dans le cas d'une mod√©lisation d'une relation entre `X` et `Y`, le biais d'un estimateur est son √©cart avec sa "vraie" valeur si on observait parfaitement la relation entre ces variables. On entend donc le biais comme __l'√©cart entre la fonction mod√©lis√©e et la fonction th√©orique__ qui permettrait de restituer le lien entre `X` et `Y`. 

Une mani√®re de jouer sur le biais d'un mod√®le c'est de modifier sa variance. La variance est une mesure de dispersion de valeurs, qui donne une estimation de l'√©cart de celles-ci √† leur moyenne. La variance d'un mod√®le estime **√† quel point celui-ci fluctue autour de sa moyenne pour coller aux donn√©es**. Une mesure utilis√©e couramment dans le cas des r√©gressions lin√©aires est le coefficient de d√©termination R2. Celui-ci calcule **la part de la variance des donn√©es expliqu√©e par la variance du mod√®le**. Autrement dit, plus mon mod√®le sera proche des points de mes donn√©es, plus sa variance et donc le R2 seront √©lev√©s. Pour illustrer ce concept, on pr√©sente plusieurs mod√®les appliqu√©s au m√™me jeu de donn√©es avec une variance plus ou moins √©lev√©e :

![Pelican](../images/biais_variance/unnamed-chunk-1-1.png)<!-- -->

Pour chaque mod√®le, la courbe du mod√®le est celle qui appara√Æt en rouge et on a mis en √©vidence en vert la projection de chaque point sur sa valeur pr√©dite par le mod√®le. Voyons comment interpr√©ter ces graphiques :  
  
- Le premier mod√®le est un mod√®le na√Øf qui se contente de pr√©dire que pour chaque valeur de `X`, `Y` sera √©gale √† sa moyenne. Par d√©finition donc, sa variance est nulle et sa capacit√© pr√©dictive faible.  
- Le second mod√®le est une r√©gression lin√©aire simple qui a un R2 d'environ 50%. Il a donc une meilleure qualit√© pr√©dictive que le premier mod√®le du fait qu'il capte une partie de la variance des donn√©es, ici √† travers une corr√©lation positive entre `X` et `Y`.  
- Le troisi√®me mod√®le est un mod√®le polynomial dont on voit qu'il est plus ajust√© que le second. Les points pr√©dits (en vert) par la courbe sont en effet plus proche des points que pour le pr√©c√©dent mod√®le et m√©caniquement cela fait augmenter le R2. Le fait d'utiliser un mod√®le polynomial a donn√© au mod√®le une plus grande souplesse ce qui lui a permis de se rapprocher de certains points extr√™mes qui √©taient √©loign√©s de la droite de r√©gression du second mod√®le.  

Ainsi, plus la variance augmente, plus le mod√®le pr√©dit en moyenne des valeurs proches de leurs vraies valeurs, ce qui fait diminuer le biais, puisqu'il est d√©fini comme l'√©cart entre notre fonction de pr√©diction et une fonction qui permettrait de pr√©dire parfaitement les donn√©es observ√©es.  

> üëã Nous c'est Antoine et Louis de Statoscop, une coop√©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  

# Enjeux de l'arbitrage biais variance

D'apr√®s ce qu'on a vu, pourquoi alors ne pas simplement chercher √† maximiser la variance pour minimiser le biais, c'est-√†-dire son √©cart aux vraies valeurs? Tout simplement parce que dans le cas de la construction d'un mod√®le de pr√©diction, nous mod√©lisons des relations entre des donn√©es √† partir d'un √©chantillon pour pr√©dire un r√©sultat sur une nouvelle population. C'est donc la performance de ce mod√®le sur de nouvelles donn√©es qui va nous int√©resser. Or, comme vous avez pu le pressentir en observant les graphiques pr√©c√©dents, __un mod√®le avec une variance tr√®s √©lev√©e se g√©n√©ralise mal √† de nouvelles donn√©es__. D'un autre c√¥t√©, __un mod√®le avec une faible variance aura lui aussi une qualit√© pr√©dictive tr√®s faible__ car il captera mal une √©ventuelle relation entre les variables.

Tout le probl√®me de cet arbitrage (ou dilemme) biais variance est donc de **trouver un mod√®le qui ait une variance suffisamment forte pour limiter le biais mais suffisamment faible pour qu'il soit g√©n√©ralisable**. Les mod√®les pr√©c√©dents avaient √©t√© entra√Æn√©s sur une base de donn√©es qui √©tait un √©chantillon al√©atoire correspondant √† 10% des donn√©es d'un √©chantillon plus important. Pour mesurer la qualit√© pr√©dictive de ces mod√®les on les applique donc aux donn√©es enti√®res et on calcule l'√©cart moyen au carr√© de la pr√©diction √† la vraie valeur, c'est-√†-dire l'erreur quadratique moyenne ou en anglais **MSE** pour *Mean Squared Error* :

![Pelican](../images/biais_variance/unnamed-chunk-2-1.png)<!-- -->

On constate que, comme attendu, le mod√®le avec le plus de variance se g√©n√©ralise mal √† des donn√©es non connues et pr√©sente une erreur de pr√©diction sup√©rieure √† celle du mod√®le lin√©aire simple. En effet, les variations de la courbe polynomiale qui lui permettait de coller au plus pr√®s des donn√©es sur lesquelles elle a √©t√© construite entra√Æne beaucoup de pr√©dictions tr√®s √©loign√©es de la vraie valeur des nouvelles donn√©es.  
L'erreur attendue d'un mod√®le de pr√©diction sur des donn√©es sur lesquelles il ne s'est pas entra√Æn√© peut en effet se d√©composer en __la somme du biais au carr√© et de la variance de ce mod√®le, c'est la d√©composition biais-variance de l'erreur quadratique__. Si vous pr√©f√©rez vous en convaincre avec la formule et la d√©monstration math√©matique, vous pouvez vous r√©f√©rer √† [la page wikip√©dia qui en parle](https://fr.wikipedia.org/wiki/Dilemme_biais-variance#D√©composition_biais-variance_de_l'erreur_quadratique). Comme le biais diminue avec la variance, il faut donc trouver un niveau de complexit√© du mod√®le qui permette √† la fois de minimiser l'√©cart √† la vraie valeur (faible biais en augmentant la variance) et d'√™tre g√©n√©ralisable en dehors de son √©chantillon d'entra√Ænement (faible variance).

# Liens avec l'overfitting et l'underfitting dans les mod√®les de Machine Learning

## Appelons un chat un chat

Le principe de l'apprentissage automatique supervis√© est le m√™me que celui qu'on a pr√©sent√© jusqu'√† maintenant : on renseigne au mod√®le des variables explicatives (`X`) et une variable d'int√©r√™t (`Y`) qu'on aimerait pouvoir ensuite pr√©dire √† partir de nouvelles donn√©es `X`. Par exemple, on renseigne des photos de chats et de chiens √©tiquett√©es : `Y` est alors le label "chien" ou "chat" de la photo, `X` la matrice de pixels de la photo. Le mod√®le devra √™tre ensuite capable de pr√©dire √† partir d'une photo qu'il n'a jamais vue si celle-ci repr√©sente un chat ou un chien. Pour mesurer la qualit√© pr√©dictive de notre mod√®le, on r√©serve des donn√©es labellis√©es sur lesquelles il ne s'entra√Ænera pas. On va ensuite lui demander de pr√©dire les labels d√©j√† connus de ces donn√©es, ce qui va nous permettre d'√©valuer la qualit√© de ces pr√©dictions. Cet √©chantillon est en g√©n√©ral appel√© **√©chantillon test** (*test set* en anglais), et les donn√©es sur lesquelles le mod√®le est entra√Æn√© s'appelle l'**√©chantillon d'entra√Ænement** (*train set*). En g√©n√©ral, si l'on dispose de suffisamment de donn√©es, on n'aura pas trop de mal √† construire un mod√®le qui parviendra √† labelliser quasiment parfaitement **nos donn√©es d'entra√Ænement**. Tout le probl√®me est d'avoir un mod√®le qui se g√©n√©ralise correctement √† de nouvelles donn√©es.

## Courbe d'apprentissage d'un mod√®le

Pour sch√©matiser, imaginons que dans les donn√©es d'entra√Ænement les chiens soient en g√©n√©ral photographi√©es √† l'ext√©rieur et les chats √† l'int√©rieur. Si mon mod√®le a une forte variance, cela signifie qu'il va prendre en compte beaucoup de d√©tails de la photo. Il va par exemple donner du poids aux √©l√©ments de fond dans sa pr√©diction et sera incapable de labelliser correctement un chien photographi√© √† l'int√©rieur. Un mod√®le bien plus basique qui se serait appuy√© par exemple uniquement sur la forme des oreilles de l'animal aurait peut-√™tre de meilleurs r√©sultats.  
Dans le premier cas, on dit que l'on est dans une situation de **surapprentissage, ou overfitting**. Le mod√®le a int√©gr√© des √©l√©ments anecdotiques, du bruit, dans son processus d√©cisionnel et cela va r√©duire sa performance pr√©dictive sur des donn√©es non connues. Autrement dit, **sa variance est trop √©lev√©e**. Si au contraire le mod√®le est trop peu complexe et n'a pas int√©gr√© assez d'informations pour diff√©rencier un chat d'un chien m√™me sur les donn√©es d'apprentissage, on dit qu'il est dans un √©tat de **sous-apprentissage, ou underfitting**.  
**Dans les deux cas, le mod√®le va avoir de mauvaises performances pr√©dictives sur des nouvelles donn√©es**. Ce constat peut √™tre sch√©matis√© de la mani√®re suivante :

![Pelican](../images/biais_variance/courb_apprent.png)  

Notons bien ici que cette relation entre complexit√© du mod√®le et d√©composition de l'erreur de pr√©diction s'entend __√† taille d'√©chantillon fix√©e__. Ainsi, plus l'√©chantillon sera grand, plus le mod√®le pourra √™tre complexe avant d'entrer dans la phase de surapprentissage.  

# Conclusion  
Dans le cadre d'une __d√©marche pr√©dictive__, il est donc tout √† fait naturel de limiter l'apprentissage du mod√®le pour optimiser ses capacit√©s pr√©dictives. Cela peut √™tre fait en r√©duisant le nombre de variables pr√©dictives utilis√©es, en limitant le nombre de couches de neurones dans un r√©seau, mais aussi en utilisant des m√©thodes de r√©duction des dimensions, comme [nous vous l'avions montr√© avec l'analyse en composantes principales](https://blog.statoscop.fr/acp-python.html). Bien s√ªr, dans une __d√©marche explicative__, il sera au contraire normal de sacrifier √©ventuellement une meilleure capacit√© pr√©dictive pour mettre en √©vidence une relation avec une variable explicative. Si vous souhaitez aller plus loin, vous pouvez parcourir l'excellente √©tude de [Shmueli, 2010](https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf) qui expose les enjeux crois√©s des d√©marches explicative et pr√©dictive dans la mod√©lisation.  
Il me reste √† remercier [Emmanuel Paroissien](https://twitter.com/EParoissien), chercheur √† l'Inra, pour nos √©changes qui m'ont aid√© √† construire cette note. N'h√©sitez pas √† [visiter notre site](https://www.statoscop.fr) et √† nous suivre sur [Twitter](https://twitter.com/stato_scop) et [Linkedin](https://www.linkedin.com/company/statoscop). Pour retrouver le code ayant servi √† g√©n√©rer cette note, vous pouvez vous rendre sur le [d√©p√¥t github de nos notes de blog](https://github.com/Statoscop/notebooks-blog).  


<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  