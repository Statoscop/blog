Title: Les s√©ries temporelles avec Python (2/4) - Visualisations et op√©rations sur les s√©ries temporelles
Author: Louis
Date: '2021-05-27'
Slug: timeseries-2
Category: Python, S√©ries Temporelles
Tags: Python, Machine Learning, Statistiques, Data Science, S√©ries temporelles, Datetime
Cover: images/cover_4.png
Summary: Quelques op√©rations sur les s√©ries temporelles, illustr√© par une petite √©tude de cas √† bicyclette.

[TOC]

Cet article est le second de notre s√©rie sur les donn√©es temporelles :  

1. Introduction √† la manipulation de donn√©es temporelles avec Python
2. **Visualisations et op√©rations sur les s√©ries temporelles**
3. √âl√©ments th√©oriques et exemples
4. Analyse, mod√©lisation et pr√©diction

Il s'int√©resse dans un premier temps √† la visualisation et aux op√©rations que l'on peut effectuer sur ces objets avant de conclure sur un petit exemple en utilisant les outils pr√©sent√©s dans ces 2 premiers posts.  

# Un peu d'anglicisme : Resampling, Shifting, and Windowing

- *Resampling* = r√©√©chantillonnage
- *Shifting* = d√©placement
- *Windowing* = fen√™trage

La capacit√© √† utiliser les dates/times comme indices pour organiser et acc√©der aux donn√©es est le fondement des outils de s√©ries temporelles sur Pandas. Les avantages de l'indexation (alignement, slicing, etc...) sont conserv√©s et Pandas fournit par ailleurs plusieurs op√©rations sp√©cifiques aux s√©ries temporelles.

On va donc d√©velopper ici quelques unes de ces op√©rations merveilleuses en utilisant comme premier exemple le cours de l'action Google en bourse (donn√©es r√©cup√©r√©es sur <a href="https://fr.finance.yahoo.com/quote/GOOG/history?p=GOOG" target="_blank">Yahoo finance</a>).

Petite pr√©cision en passant, le terme "s√©ries temporelles" d√©signe en g√©n√©ral, dans le contexte `Pandas`, un objet `Series` index√© par un `DatetimeIndex`.

```python
import pandas as pd
# parse_dates=True permet √† pandas de rep√©rer les dates sous diff√©rents formats
goog = pd.read_csv('data/GOOG.csv', index_col='Date', parse_dates=True)
goog.head(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Open</th>
      <th>High</th>
      <th>Low</th>
      <th>Close</th>
      <th>Adj Close</th>
      <th>Volume</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2004-08-19</th>
      <td>49.813290</td>
      <td>51.835709</td>
      <td>47.800831</td>
      <td>49.982655</td>
      <td>49.982655</td>
      <td>44871361</td>
    </tr>
    <tr>
      <th>2004-08-20</th>
      <td>50.316402</td>
      <td>54.336334</td>
      <td>50.062355</td>
      <td>53.952770</td>
      <td>53.952770</td>
      <td>22942874</td>
    </tr>
  </tbody>
</table>
</div>




```python
goog.tail(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Open</th>
      <th>High</th>
      <th>Low</th>
      <th>Close</th>
      <th>Adj Close</th>
      <th>Volume</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-05-21</th>
      <td>2365.98999</td>
      <td>2369.00000</td>
      <td>2342.370117</td>
      <td>2345.100098</td>
      <td>2345.100098</td>
      <td>1139600</td>
    </tr>
    <tr>
      <th>2021-05-24</th>
      <td>2367.00000</td>
      <td>2418.47998</td>
      <td>2360.110107</td>
      <td>2406.669922</td>
      <td>2406.669922</td>
      <td>1061400</td>
    </tr>
  </tbody>
</table>
</div>



Ces donn√©es recensent certaines informations sur l'action Google du 19 ao√ªt 2004 au 24 mai 2021 : les prix √† l'ouverture et √† la cl√¥ture, le maximum et le minimum sur la journ√©e, les prix ajust√©s et les volumes. On √©tudie ici la s√©rie temporelle des prix √† la cl√¥ture.


```python
goog = goog['Close']
goog.index
```




    DatetimeIndex(['2004-08-19', '2004-08-20', '2004-08-23', '2004-08-24',
                   '2004-08-25', '2004-08-26', '2004-08-27', '2004-08-30',
                   '2004-08-31', '2004-09-01',
                   ...
                   '2021-05-11', '2021-05-12', '2021-05-13', '2021-05-14',
                   '2021-05-17', '2021-05-18', '2021-05-19', '2021-05-20',
                   '2021-05-21', '2021-05-24'],
                  dtype='datetime64[ns]', name='Date', length=4220, freq=None)



C'est bien une s√©rie index√©e par un `DatetimeIndex` que l'on affiche avec style en fixant les param√®tres d'affichage par d√©faut comme √©tant ceux de `seaborn` : 


```python
import matplotlib.pyplot as plt
import seaborn as sns ; sns.set() #pour d√©finir les param√®tres d'affichage de seaborn par d√©faut
plt.rcParams["figure.figsize"] = (12,8)
```


```python
goog.plot();
```


    
![Pelican](../images/SeriesTemp2/output_9_0.png)
    


## R√©√©chantillonage et conversion de fr√©quences

Une manipulation classique des s√©ries temporelles est le r√©√©chantionnage (resampling) √† une fr√©quence plus ou moins haute. Cela consiste √† augmenter ou diminuer la fr√©quence des observations. Il y a donc 2 possibilit√©s :  
- si on augmente la fr√©quence cela veut dire ajouter des points et dans ce cas il faut d√©finir quelle strat√©gie utiliser pour interpoler les nouveaux points (un exemple de strat√©gie basique est de r√©p√©ter la derni√®re valeur) ;  
- si on diminue la fr√©quence, ce qui est le cas le plus classique, on va supprimer des points et il faut l√† aussi d√©terminer la strat√©gie √† utiliser. Deux options sont possibles : on s√©lectionne uniquement les points correspondants √† la nouvelle fr√©quence plus faible ou bien on agr√®ge les points entre 2 fr√©quences en utilisant une fonction d'aggr√©gation comme par exemple une moyenne, une m√©diane, un max, etc...

Pour ce faire, `pandas` dispose de deux m√©thodes qui sont `resample()` ou `asfreq()`. La diff√©rence entre les deux est que `resample` consiste √† agr√©ger toutes les donn√©es comprises entre 2 multiples de la fr√©quence alors que `asfreq` s√©lectionne la valeur correspondant √† la fr√©quence. Aussi, `resample` renvoie un objet particulier qui est un `DatetimeIndexResampler` sur lequel il faut appliquer une m√©thode d'aggr√©gation ou d'imputation pour r√©cup√©rer une s√©rie. La m√©thode `asfreq` retourne directement une s√©rie.

On va de ce pas illustrer avec la s√©rie Google en diminuant la fr√©quence afin de n'avoir qu'un point par ann√©e. On va prendre le dernier jour ouvrable de l'ann√©e (jour ouvrable car la s√©rie prend des valeurs uniquement pour les jours ouvrables).

Petit rappel, on utilise ci-dessous, le code de fr√©quence `BA` pour r√©cup√©rer le dernier jour ouvrable de l'ann√©e mais pour en savoir plus sur les fr√©quences, <a href="https://blog.statoscop.fr/timeseries-1.html" target="_blank">l'√©pisode 1 de cette s√©rie</a> vous en apprendra davantage.


```python
goog.plot(alpha=0.4)
goog.resample('BA').mean().plot()
goog.asfreq('BA').plot();
plt.legend(['close', 'resample', 'asfreq'], loc='upper left');
```


    
![Pelican](../images/SeriesTemp2/output_14_0.png)
    


Pour un resampling avec une fr√©quence plus importante, `resample()` et `asfreq()` sont √©quivalentes. Par d√©faut, les 2 m√©thodes laissent les valeurs non existantes vides. Toutefois, `asfreq()` accepte un param√®tre `method` dans lequel on peut sp√©cifier comment imputer les valeurs manquantes g√©n√©r√©es par l'augmentation de la fr√©quence. C'est faisable aussi avec `resample` en utilisant les m√©thodes `bfill` ou `ffill` des objets `pandas.core.resample.Resampler`.

Le petit bout de code ci-dessous selectionne uniquement les 14 derniers jours et effectue un resampling de la s√©rie avec une fr√©quence quotidienne (cela inclue donc les weekends !). Ensuite on affiche pour chaque m√©thode (`resample()` et `asfreq()`) les courbes r√©√©chantillonn√©es sans imputer les valeurs manquantes, avec une imputation de type `bfill` qui impute la premi√®re valeur suivante non manquante (donc dans ce cas celle du lundi) et de type `ffill` qui impute la derni√®re valeur (donc ici, celle du vendredi).


```python
data = goog.iloc[-14:]

#visualisation
fig, ax = plt.subplots(2,2, sharex=True, sharey=True, figsize=(14,9))

#avec asfreq
data.asfreq('D').plot(ax=ax[0,0], marker='o')
ax[0,0].set_title("asfreq()", fontsize=20);

data.asfreq('D', method='bfill').plot(ax=ax[1,0], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1,0], style='--o')
ax[1,0].legend(["back-fill", "forward-fill"]);

#avec resample
data.resample('D').mean().plot(ax=ax[0,1], marker='o')
ax[0,1].set_title("resample()", fontsize=20);

data.resample('D').bfill().plot(ax=ax[1,1], style='-o')
data.resample('D').ffill().plot(ax=ax[1,1], style='--o')
ax[1,1].legend(["back-fill", "forward-fill"]);
```


    
![Pelican](../images/SeriesTemp2/output_16_0.png)
    


## D√©placements

Une autre op√©ration fondamentale de l'analyse des donn√©es temporelles est le d√©placement ou d√©calage (on parle plus souvent de *time-shifts* ou *shifting*)

On utilise pour cette op√©ration la m√©thode `shift()` dont le principe est de d√©placer les valeurs par rapport aux indices. Le d√©calage doit bien s√ªr √™tre un multiple de la fr√©quence !


```python
fig, ax = plt.subplots(3, sharey=True)

goog = goog.asfreq('D', method='ffill')

goog.plot(ax=ax[0])
goog.shift(900).plot(ax=ax[1])
goog.shift(-900).plot(ax=ax[2])

local_max = pd.to_datetime('2010-01-01')
offset1 = pd.Timedelta(900, 'D')
offset2 = pd.Timedelta(-900, 'D')

ax[0].legend(['close'], loc=2)
ax[0].get_xticklabels()[3].set(weight='heavy', color='red')
ax[0].axvline(local_max, alpha=0.3, color='red')

ax[1].legend(['shift(900)'], loc=2)
ax[1].get_xticklabels()[3].set(weight='heavy', color='red')
ax[1].axvline(local_max + offset1, alpha=0.3, color='red')

ax[2].legend(['shift(-900)'], loc=2)
ax[2].get_xticklabels()[3].set(weight='heavy', color='red')
ax[2].axvline(local_max + offset2, alpha=0.3, color='red');
```


    
![Pelican](../images/SeriesTemp2/output_18_0.png)
    


Une utilisation possible du *shifting* est par exemple de calculer le retour sur investissement √† 1 an de l'action de Google (ROI - *return on investment*, par ici <a href="https://en.wikipedia.org/wiki/Return_on_investment" target="_blank">Wiki</a>).


```python
ROI_1 = 100 * (goog.shift(-365) / goog - 1)
ROI_1.iloc[:-365].plot()
plt.ylabel('ROI');
```


    
![Pelican](../images/SeriesTemp2/output_20_0.png)
    


Qu'en conclure ? Pour les boursicoteurs, vous avez rat√© le coche, fallait acheter en 2004 ou en 2009.

## Attention, fen√™tres glissantes

Enfin, la 3√®me op√©ration classique des s√©ries temporelles consiste √† calculer diff√©rentes statistiques sur une fen√™tre d'une longueur donn√©e et qui se d√©place. On parle plus de *rolling window* que de fen√™tres glissantes...et pour ce faire, Pandas a tout ce qu'il faut avec la m√©thode `rolling()` pour les objets `Series` et `DataFrame`. Regardons d'un peu plus pr√®s en calculant avec la m√©thode rolling la moyenne annuelle centr√©e et l'√©cart-type annuel centr√©.


```python
rol = goog.rolling(365, center=True)

data = pd.DataFrame({'close': goog,
                     'moyenne': rol.mean(),
                     'std': rol.std()})
ax = data.plot(style=['-', '--', ':'])
ax.lines[0].set_alpha(0.4)
```


    
![Pelican](../images/SeriesTemp2/output_23_0.png)
    
> üëã Nous c'est Antoine et Louis de Statoscop, une coop√©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>    


# Un exemple de visualisation : le nombre de v√©los √† Paris Montparnasse

On va terminer sur un petit exemple un peu plus parlant, ou en tout cas, un peu moins financier, en regardant le nombre de v√©los pass√©s devant un des compteurs de la ville de Paris, situ√© sur le boulevard Montparnasse. Le jeu de donn√©es vient <a href="https://opendata.paris.fr/explore/dataset/comptage-velo-donnees-compteurs/information/?disjunctive.id_compteur&disjunctive.nom_compteur&disjunctive.id&disjunctive.name" target="_blank">de l√†</a>. On r√©cup√®re ainsi le d√©compte horaire des v√©los.

## Nettoyage des donn√©es 

Un premier coup d'oeil sur les donn√©es nous permet de voir qu'il y a beaucoup de colonnes inutiles dans ce dataset. On va donc se contenter de ce qui nous int√©resse : le timestamp et le nombre de v√©los. Avec un petit peu de nettoyage directement au moment de l'import dans le `pandas.read_csv`, √ßa donne :


```python
velo = pd.read_csv('data/comptage-velo-donnees-compteurs.csv', sep=';',
                   names=["nb", "date"], header=0,
                   usecols=[4,5])
velo.head(3)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>nb</th>
      <th>date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>21.0</td>
      <td>2020-04-01T07:00:00+02:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21.0</td>
      <td>2020-04-01T09:00:00+02:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.0</td>
      <td>2020-04-01T12:00:00+02:00</td>
    </tr>
  </tbody>
</table>
</div>



En important le jeu de donn√©es, on voit ce "+02:00" qui d√©finit en fait la timezone. Plusieurs solutions possibles pour g√©rer ce probl√®me :  
- on peut utiliser les m√©thodes de manipulation de timezone avec `tz_convert` et `tz_localize`  
- on peut d√©finir notre propre parser de date au moment de l'import en supprimant le "+02:00" avec un `split` par exemple.

Ci-dessous, ces 2 approches mises en oeuvre.


```python
pd.DatetimeIndex(pd.to_datetime(velo.date, utc=True)).tz_convert('Europe/Paris').tz_localize(None)
```




    DatetimeIndex(['2020-04-01 07:00:00', '2020-04-01 09:00:00',
                   '2020-04-01 12:00:00', '2020-04-01 15:00:00',
                   '2020-04-01 16:00:00', '2020-04-01 19:00:00',
                   '2020-04-01 20:00:00', '2020-04-01 21:00:00',
                   '2020-04-01 22:00:00', '2020-04-01 23:00:00',
                   ...
                   '2021-05-22 20:00:00', '2021-05-23 00:00:00',
                   '2021-05-23 02:00:00', '2021-05-23 03:00:00',
                   '2021-05-23 04:00:00', '2021-05-23 16:00:00',
                   '2021-05-23 19:00:00', '2021-05-24 02:00:00',
                   '2021-05-24 05:00:00', '2021-05-24 14:00:00'],
                  dtype='datetime64[ns]', name='date', length=10046, freq=None)




```python
velo = pd.read_csv('data/comptage-velo-donnees-compteurs.csv', sep=';',
                   names=["nb", "date"], header=0,
                   usecols=[4,5],
                   index_col="date",
                   parse_dates=True,
                   date_parser=lambda s: pd.to_datetime(s.split("+")[0])
                  )
# read_csv retourne un dataframe or on veut un objet Series, donc puisqu'on le veut, et qu'on l'a, ben on le prend.
velo = velo.nb
velo[:3]
```




    date
    2020-04-01 07:00:00    21.0
    2020-04-01 09:00:00    21.0
    2020-04-01 12:00:00    14.0
    Name: nb, dtype: float64



Regardons √† quoi ressemble cette s√©rie temporelle du nombre de v√©los sur le boulevard Montparnasse ! 


```python
velo.plot(legend=False)
plt.ylabel('D√©compte horaire des v√©los');
```


    
![Pelican](../images/SeriesTemp2/output_32_0.png)
    


On voit pas grand chose pour le moment mais on d√©tecte d√©j√† un probl√®me de valeurs manquantes au niveau du mois de f√©vrier 2021. On va donc prendre les donn√©es de 2020 uniquement pour s'√©pargner la gestion de ces donn√©es manquantes.


```python
velo = velo['2020']
velo.plot(legend=False, figsize=(18,8))
plt.ylabel('D√©compte horaire des v√©los');
```


    
![Pelican](../images/SeriesTemp2/output_34_0.png)
    

## Analyse des donn√©es 

La s√©rie par heure √©tant trop "dense" pour √™tre clairement lisible, on va diminuer la fr√©quence avec un `resample` pour faire la somme des v√©los sur une journ√©e et sur une semaine.


```python
fig, ax = plt.subplots(1,2,figsize=(18,7))

# Somme sur une journ√©e
velo_jr =  velo.resample('D').sum()
velo_jr.plot(ax=ax[0], legend=False)
ax[0].set_ylabel('D√©compte journalier des v√©los');

# Somme sur une semaine
velo_sem =  velo.resample('W').sum()
velo_sem.plot(ax=ax[1], legend=False)
ax[1].set_ylabel('D√©compte hebdo des v√©los');
```


    
![Pelican](../images/SeriesTemp2/output_36_0.png)
    


Avec ces r√©√©chantillonages on veut une tendance annuelle qui se d√©gage avec notamment un pic de reprise d'activit√© au printemps, apr√®s le confinement et une baisse de fr√©quentation pendant l'√©t√©. De la m√™me mani√®re, la baisse significative visible au mois de novembre est certainement d√ªe au second confinement de 2020.

Une autre information visible sur le graphique des d√©comptes journaliers est la baisse du nombre de v√©lo environ 4 fois par mois, cela correspond certainement aux weekends mais ne nous avan√ßons pas trop...

On va regarder avec la m√©thode `rolling`, la moyenne mobile mensuelle et faire jouer certains param√®tres afin de voir ce qu'il en est.


```python
plt.figure(figsize=(20,12))
plt.plot(velo_jr.rolling(30, center=True).mean(),label="unweighted")

for std in [1,5,10,15,20,30]:
    plt.plot(velo_jr.rolling(30, center=True, win_type='gaussian').mean(std=std), label=f"win{std}")

plt.legend(loc='best');
```


    
![Pelican](../images/SeriesTemp2/output_38_0.png)
    


Le param√®tre `win_type="gaussian"` permet d'appliquer une pond√©ration au calcul de la moyenne. En l'occurence on applique des poids qui suivent une loi normale dont l'√©cart-type est d√©fini dans la fonction d'aggr√©gation `mean`. Plus cet √©cart-type est faible, plus les jours proches comptent et ceux √©loign√©s ne comptent pas. C'est le cas de la courbe orange qui a donc tendance √† suivre d'assez pr√®s la courbe initiale. En revanche, si l'on augmente l'√©cart-type, on prend en compte plus de jours autour et on lisse ainsi les r√©sultats. Pour finir, si l'√©cart-type est tr√®s √©lev√©, alors on pond√®re de mani√®re quasi identique les 30 jours de la fen√™tre et on retombe donc sur une moyenne non-pond√©r√©e.

On va regarder maintenant comment √ßa se passe au niveau hebdomadaire pour comprendre comment √©volue la fr√©quentation selon les diff√©rents horaires d'une journ√©e et selon les diff√©rents jours d'une semaine. Pour cela, on va pouvoir utiliser les attributs `time` et `dayofweek` des objets `DatetimeIndex` afin d'afficher les d√©comptes de v√©los par heure de la journ√©e et par jour de la semaine.


```python
import numpy as np
fig, ax = plt.subplots(1,2,figsize=(18,7))

par_hr = velo.groupby(velo.index.time).mean()
heures = 4 * 60 * 60 * np.arange(6)
par_hr.plot(ax=ax[0], xticks=heures, legend=False);

par_sem = velo.groupby(velo.index.dayofweek).mean()
par_sem.index = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']
par_sem.plot(ax=ax[1], legend=False);
```


    
![Pelican](../images/SeriesTemp2/output_40_0.png)
    


On retrouve bien certaines informations qu'on avait √©voqu√©es :   
- baisse de la fr√©quentation le weekend et sur une journ√©e,  
- pics de fr√©quentation √† 8h et √† 18h et l√©g√®re augmentation √† l'heure du d√©jeuner.

Cette 2√®me information est certes logique pour les jours ouvrables mais c'est plus √©tonnant pour les jours de weekends...allons voir de plus pr√®s !


```python
jours_ouvrables = np.where(velo.index.dayofweek < 5, 'Ouvrable', 'Weekend')
par_hr = velo.groupby([jours_ouvrables, velo.index.time]).mean()
```


```python
fig, ax = plt.subplots(1,2,figsize=(18,7))
par_hr.loc['Ouvrable'].plot(ax=ax[0], title='Jours ouvrables', xticks=heures, legend=False)
par_hr.loc['Weekend'].plot(ax=ax[1], title='Weekends', xticks=heures, legend=False);
```


    
![Pelican](../images/SeriesTemp2/output_43_0.png)
    


C'est tout de suite plus clair : le weekend, les gens dorment et sortent se promener l'apr√®s-midi ! Nous voil√† rassur√©s. Sur cette belle d√©couverte, on se dit √† tr√®s vite pour le post num√©ro 3 de cette s√©rie ! Comme d'habitude, vous pouvez retrouver l'ensemble du notebook ayant servi √† g√©n√©rer cette note sur le <a href="https://github.com/Statoscop/notebooks-blog" target="_blank">github de Statoscop</a>.
  

<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  