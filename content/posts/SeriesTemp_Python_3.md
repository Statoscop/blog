Title: Les s√©ries temporelles avec Python (3/4) - √âl√©ments th√©oriques et exemples
Author: Louis
Date: '2021-06-10'
Slug: timeseries-3
Category: Python, S√©ries Temporelles
Tags: Python, Machine Learning, Statistiques, Data Science, S√©ries temporelles, Datetime
Cover: images/cover_6.png
Summary: √âl√©ments th√©oriques g√©n√©raux sur les s√©ries temporelles et quelques exemples classiques.

[TOC]

>1. Introduction √† la manipulation de donn√©es temporelles avec Python
>2. Visualisation et op√©rations sur les s√©ries temporelles
>3. **√âl√©ments th√©oriques et exemples**
>4. Analyse, mod√©lisation et pr√©diction

Dans ce troisi√®me post, nous entrons dans le coeur du sujet avec l'introduction des notions fondamentales des s√©ries temporelles puis nous illustrerons ces concepts par quelques "vraies" s√©ries de donn√©es.

# Notions de bases

## D√©finition

Une s√©rie temporelle (ou chronologique), est une suite de valeurs num√©riques repr√©sentant l‚Äô√©volution d‚Äôune quantit√© au cours du temps. On la note g√©n√©ralement $\{y_t\}_{t=1}^{T}$ c'est-√†-dire l'ensemble des $T$ valeurs $y_1, y_2, y_3,..., y_{T-1}, y_T$.  

On rencontre parfois les termes "processus" ou "processus stochastique". Cela fait r√©f√©rence au m√©canisme, **inconnu et non observable**, qui a entra√Æn√© ces observations.

## Les composantes d'une s√©rie temporelle

Une s√©rie temporelle peut se d√©composer en 4 composantes :  
- **Tendance** : orientation g√©n√©rale de la s√©rie (vers le haut ou vers le bas)  
- **Saisonnalit√©** : tendances hebdomadaires, mensuelles, trismestrielles ou annuelles  
- **Cycle** : cycles √©conomiques √† long terme, sur plusieurs ann√©es  
- **Bruit** : ce qui reste apr√®s avoir extrait les composants pr√©c√©dents  

## La stationnarit√©

Un processus $\{y_t\}_{t=1}^{T}$ est dit stationnaire lorsque :  
- $E(y_t) = \mu$  
- $V(y_t) = \sigma¬≤$  
- $\gamma(t,s) = \gamma(t-s)$ o√π $\gamma$ est la fonction d'autocorr√©lation qu'on va d√©couvrir de suite  

Ok c'est joli les formules, mais concr√®tement √ßa veut dire quoi qu'une s√©rie est stationnaire ?

Premi√®rement, la moyenne de la s√©rie est constante et ne d√©pend donc pas du temps. La courbe rouge ci-dessous n'est d√®s lors pas stationnaire puisque sa moyenne augmente en fonction du temps

<img src='../images/SeriesTemp3/station1.png' width=700>

Deuxi√®mement, la variance de la s√©rie est constante et ne d√©pend donc pas du temps. On parle **d'homosc√©dasticit√©**. La courbe rouge ci-dessous n'est d√®s lors pas stationnaire puisque l'√©cart des points √† leur moyenne (repr√©sent√©e par la droite bleue) varie au cours du temps.

<img src='../images/SeriesTemp3/station2.png' width=700>

Troisi√®mement, la covariance entre 2 points ne d√©pend pas du temps mais uniquement de l'√©cart entre ces points. La courbe rouge ci-dessous n'est d√®s lors pas stationnaire puisque l'espacement entre les points varie au cours du temps (ceux du milieu sont beaucoup plus proches entre eux que ceux aux extremit√©s).

<img src='../images/SeriesTemp3/station3.png' width=700>


## L'autocorr√©lation et l'autocorr√©lation partielle

L'autocorr√©lation est un indicateur montrant comment les observations d'une s√©rie temporelle sont li√©es entre elles. Il s'agit de la corr√©lation entre la s√©rie et cette m√™me s√©rie d√©cal√©e dans le temps :
$$\gamma(t,s) = \frac{1}{\sigma¬≤} Cov(y_t,y_s) = \frac{1}{\sigma¬≤} E[(y_t-\mu)(y_s-\mu)]$$

Une autre notation qui peut para√Ætre plus claire et qui fait justement intervenir le d√©calage $k$ consid√©r√© entre les 2 s√©ries $\{y\}$ :
$$\gamma(k) = \frac{1}{\sigma¬≤} Cov(y_t,y_{t+k}) = \frac{1}{\sigma¬≤} E[(y_t-\mu)(y_{t+k}-\mu)]$$

Si on ne normalise pas par $\sigma¬≤$ on retrouve l'autocovariance. Dans le cadre des s√©ries temporelles (et plus largement du traitement du signal) on utilise un peu indistinctement les 2 termes.

Comme la corr√©lation, l'autocorr√©lation prend ses valeurs entre $[-1,1]$. Une valeur de $1$ signifie que la s√©rie et la s√©rie d√©cal√©e se superposent parfaitement.

La **fonction d'autocorr√©lation (ACF)** et la **fonction d'autocorr√©lation partielle (PACF)** permettent de mesurer l'association entre des valeurs actuelles et pass√©es. Elles indiquent les valeurs pass√©es les plus corr√©l√©es aux valeurs suivantes et donc les plus utiles √† la pr√©vision de valeurs futures. Pour parler _"avec les mains"_, la fonction ACF mesure la similitude entre 2 observations en fonction du d√©calage temporel entre les 2.

Concr√®tement :  
- ACF au d√©calage k = corr√©lation entre les valeurs s√©par√©es par k intervalles  
- PACF au d√©calage k = corr√©lation entre les valeurs s√©par√©es par k intervalles, compte tenu des valeurs des intervalles interm√©diaires  

<img src='../images/SeriesTemp3/acf_pacf.png'>

Ces graphiques, qui r√©pondent au doux nom de corr√©logramme, interviennent dans la d√©termination de l'ordre des processus d'un mod√®le ARIMA qu'on ne pr√©sente plus mais qu'on aura l'occasion de croiser prochainement c'est promis.

## Bruit blanc

Un **bruit blanc** est un processus $\{\epsilon_t\}_{t=1}^{T}$ qui v√©rifie :  
- $E[\epsilon_t] = 0$  
- $E[\epsilon_t¬≤] = \sigma¬≤$  
- $E[\epsilon_t\epsilon_s] = 0$ pour $s \neq t$  

C'est donc une s√©rie de moyenne constante et nulle, de variance constante et de covariance nulle entre 2 observations (ind√©pendamment du d√©calage entre les 2). Ce dernier point signifie donc que les observations sont **non corr√©l√©es** entre elles.

Un **bruit blanc ind√©pendant** est un processus $\{\epsilon_t\}_{t=1}^{T}$ qui v√©rifie :  
- $E[\epsilon_t] = 0$  
- $E[\epsilon_t¬≤] = \sigma¬≤$  
- $\epsilon_t$ et $\epsilon_s$ ind√©pendants pour $s \neq t$  

Les deux premi√®res hypoth√®ses sont identiques. En revanche, la 3√®me hypoth√®se est plus forte puisque l'ind√©pendance implique que la covariance soit nulle mais la r√©ciproque n'est pas vraie (<a href="https://fr.wikipedia.org/wiki/Ind%C3%A9pendance_(probabilit%C3%A9s)#Ind%C3%A9pendance_et_corr%C3%A9lation" target="_blank">Ma√Ætre Wiki</a> vous fournira un contre-exemple avec des variables al√©atoires discr√®tes ; un autre contre-exemple classique avec des variables al√©atoires continues $X \hookrightarrow \mathcal{N}(0,1)$ et $Y=X¬≤$ sont d√©pendantes et de covariance nulle).  

Un **bruit blanc gaussien** $\{\epsilon_t\}_{t=1}^{T}$ est un bruit blanc ind√©pendant qui suit une loi normale centr√©e : $\epsilon_t \hookrightarrow \mathcal{N}(0,\sigma¬≤)$  

On ajoute donc une hypoth√®se suppl√©mentaire sur la distribution qui doit √™tre une loi normale de moyenne nulle.

Intuitivement, un bruit blanc dans le cadre des s√©ries temporelles, c'est **la partie purement al√©atoire du processus**. Effectivement, j'imagine bien ce que vous pensez et vous avez tout √† fait raison, les bruits blancs sont des s√©ries stationnaires (moyenne, variance et covariance constantes).

Dans les lignes de code ci-dessous, on g√©n√®re un bruit blanc gaussien qu'on visualise ensuite en utilisant la fonction `ts_plot` qui est disponible sur le github de Statoscop, <a href="https://github.com/Statoscop/notebooks-blog/blob/main/Series%20temporelles/fonctions.py" target="_blank">ici pr√©cis√©ment</a> dans le fichier `fonction.py` du dossier S√©ries Temporelles.


```python
import numpy as np
e = np.random.normal(size=1000)

import fonctions as fct
fct.ts_plot(e)
```


    
![Pelican](../images/SeriesTemp3/output_3_0.png)
    


**Interpr√©tation :** les graphiques ci-dessus ressemblent √† la structure souhait√©e d'une s√©rie chronologique d'erreurs :  
- pas de pics dans les graphes ACF et PACF   
- le QQ-plot et l'histogramme indiquent un comportement normal  

Petite parenth√®se, on essaiera de faire prochainement un post sur la normalit√© des r√©sidus, l'interpr√©tation des QQ-plot ou encore les diff√©rents tests de normalit√©.

## La marche al√©atoire

Un processus de marche al√©atoire $\{x_t\}_{t=1}^{T}$ est une s√©rie qui v√©rifie :  
$$x_t = x_{t-1} + \epsilon_t$$
avec $\epsilon_t$ un bruit blanc

Ci-dessous, on simule et visualise une marche al√©atoire.


```python
#param√®tres
T = 1000
e = np.random.normal(size=T)
x = np.zeros_like(e)
 
for t in range(T):
    x[t] = x[t-1] + e[t]

fct.ts_plot(x)
```


    
![Pelican](../images/SeriesTemp3/output_5_0.png)
    


**Interpr√©tation :** on voit la ressemblance entre ce processus de marche al√©atoire et les cours d'actifs en bourse qui sont caract√©ris√©s par :   
- un ACF √©lev√© pour toute longueur de d√©calage et un PACF √©lev√© pour le premier d√©calage uniquement   
- un QQ-plot et un histogramme indiquant que la s√©rie n'est pas un bruit blanc  

La marche al√©atoire n'est clairement pas une s√©rie stationnaire. Par contre, la s√©rie diff√©renci√©e $(x_t - x_{t-1})$ est stationnaire puisqu'elle vaut $(\epsilon_t)$ qui est un bruit blanc. 

## Mod√®le autor√©gressif

Une s√©rie temporelle $\{x_t\}_{t=1}^{T}$ qui suit un mod√®le autor√©gressif d'ordre $p$ (et not√© $AR(p)$) est telle que :  
$$x_t = \mu + \Sigma_{i=1}^{p}\phi_i x_{t-i} + \epsilon_t$$
avec $\mu$ une constante et $\epsilon_t$ un bruit blanc

Concr√®tement, cela signifie que la valeur de la s√©rie au temps $t$ est une combinaison lin√©aire des $p$ valeurs pr√©c√©dentes (√† un terme d'erreur $\epsilon_t$ pr√®s).

Un processus autor√©gressif d'ordre 1 est donc donn√© par :
$AR(1)$ : $x_t = \mu + \phi x_{t-1} + \epsilon_t$ 

On simule et visualise un processus $AR(1)$ ci-dessous.


```python
#param√®tres
T = 1000
e = np.random.normal(size=T)
x = np.zeros_like(e)
phi = 0.3
 
for t in range(T):
    x[t] = phi * x[t-1] + e[t]
    
fct.ts_plot(x)
```


    
![Pelican](../images/SeriesTemp3/output_7_0.png)
    


**Interpr√©tation :**  
- pour un processus AR(1) avec $\phi=0.3$,on voit sur les graphiques ACF et PACF, un pic au premier d√©calage qui tend tout de suite vers 0.  
- l'histogramme et le QQ-plot indiquent des r√©sidus presque al√©atoires.  

*Remarque pas anodine* : la marche al√©atoire introduite pr√©c√©demment est un processus AR(1) avec $\phi=1$ et n'est pas stationnaire. Or ici, on peut voir qu'avec $\phi=0.3$, le processus AR(1) semble stationnaire. Bizarre...vous avez dit bizarre ? En fait, la stationnarit√© d'un processus $AR(1)$ d√©pend de si $|\phi|<1$. On peut g√©n√©raliser cette propri√©t√© aux processus $AR(p)$.

## Mod√®le moyenne mobile

Le mod√®le de moyenne mobile d'ordre $q$ not√© $MA(q)$ (pour *moving-average*) suppose que la s√©rie temporelle observ√©e peut √™tre repr√©sent√©e par une combinaison lin√©aire de termes d'erreur de bruit blanc. Cela s'√©crit donc :  
$$x_t = \epsilon_t + \Sigma_{i=1}^{q}\theta_i\epsilon_{t-i}$$ avec $\epsilon_t$ un bruit blanc

Et c'est parti pour la simulation et la visualisation d'un processus $MA(2)$.


```python
#param√®tres
T = 1000
e = np.random.normal(size=T)
x = np.zeros_like(e)
theta1 = 0.8
theta2 = -1.4
 
for t in range(T):
    x[t] = e[t] + theta1 * e[t-1] + theta2 * e[t-2]

fct.ts_plot(x)
```


    
![Pelican](../images/SeriesTemp3/output_9_0.png)
    


**Interpr√©tation :** pour le processus $MA(2)$ avec les param√®tres choisis, on voit un pic plus important pour le deuxi√®me d√©calage dans les trac√©s ACF et PACF. 
On peut observer √©galement que le pic dans le graphique PACF ne d√©cro√Æt pas imm√©diatement mais prend quelques p√©riodes pour se "d√©sint√©grer".

*Remarque toujours pas anodine* : en tant que combinaison lin√©aire de processus stationnaires, un processus $MA(q)$ est toujours stationnaire.

## Mod√®le autor√©gressif et moyenne mobile

√áa va commencer √† se corser un peu avec le processus $ARMA(p,q)$ qui est une combinaison des deux mod√®les pr√©c√©dents et s'√©crit donc :  
$$x_t = \epsilon_t + \Sigma_{i=1}^{p}\phi_i x_{t-i} + \Sigma_{i=1}^{q}\theta_i\epsilon_{t-i}$$
avec $\epsilon_t$ un bruit blanc

Regardons √† quoi ressemble un $ARMA(2,3)$.


```python
#param√®tres
T = 1000
e = np.random.normal(size=T)
x = np.zeros_like(e)
phi1 = 0.3
phi2 = 0.6
theta1 = 0.8
theta2 = -1.1
theta3 = 0.5

for t in range(T):
    x[t] = e[t] + phi1 * x[t-1] + phi2 * x[t-2] + theta1 * e[t-1] + theta2 * e[t-2] + theta3 * e[t-3]

fct.ts_plot(x)
```


    
![Pelican](../images/SeriesTemp3/output_11_0.png)
    


**Interpr√©tation :** on voit clairement que la s√©rie n'est pas stationnaire et que les r√©sidus ont l'air de suivre une loi normale mais on n'ira pas plus loin pour l'interpr√©tation...

Maintenant qu'on a fait un bon petit tour d'horizon th√©oriques des propri√©t√©s des s√©ries temporelles, allons en voir des vraies dans leur milieu naturel !  

> üëã Nous c'est Antoine et Louis de Statoscop, une coop√©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  

# Des exemples de s√©ries temporelles

On va simplement regarder quelques s√©ries temporelles et les commenter graphiquement.

## Des t√¢ches solaires

La s√©rie "sunspot" ci-dessous recense le nombre annuel de <a href="https://fr.wikipedia.org/wiki/Tache_solaire" target="_blank">t√¢ches solaires</a> de 1790 √† 1970.


```python
import pandas as pd
import matplotlib.pyplot as plt

sunspot = pd.read_csv("data/sunspot.csv", index_col="t")
sunspot.plot(y="sunspot", figsize=(14,4));
```


    
![Pelican](../images/SeriesTemp3/output_15_0.png)
    


Quelques remarques :  
- cette s√©rie n'est clairement pas stationnaire  
- il ne semble pas y avoir de saisonnalit√©  
- par contre, elle semble pr√©senter des cycles avec un pic plus ou moins tous les 10 ans dont l'amplitude semble l√©g√®rement augmenter au cours du temps  
- cette s√©rie ne semble pas tr√®s bruit√©e (c'est-√†-dire qu'il n'y a pas une forte variabilit√© d'un point √† l'autre qui se traduirait par des pics tr√®s rapproch√©s comme pour les s√©ries d'actifs financiers par exemple). Attention toutefois, avant de conclure l√†-dessus il faut s'assurer de la fr√©quence de la s√©rie (annuelle).  

## Des am√©ricains

La s√©rie "uspop" ci-dessous recense la population des Etats-Unis, en millions, de 1790 √† 1990 (avec une fr√©quence d√©cennale).


```python
uspop = pd.read_csv("data/uspop.csv", index_col="t")
uspop.plot(y="uspop", figsize=(14,4));
```


    
![Pelican](../images/SeriesTemp3/output_17_0.png)
    


Quelques remarques :  
- cette s√©rie n'est pas stationnaire  
- cette s√©rie est tr√®s lisse puisqu'elle ne pr√©sente ni saisonnalit√©, ni cycle, ni bruit  
- on identifie donc uniquement une tendance  

## De la bi√®re australienne

La s√©rie "beer" ci-dessous mesure la production mensuelle de bi√®re en Australie, en m√©galitres, de janvier 1956 √† ao√ªt 1995.


```python
beer = pd.read_csv("data/beer.csv", header=None, names=["t", "Beer"], parse_dates=True, dayfirst=True, index_col="t")
beer.plot(y="Beer", figsize=(14,4));
```


    
![Pelican](../images/SeriesTemp3/output_19_0.png)
    


Quelques remarques :  
- cette s√©rie n'est pas stationnaire  
- il y a une tendance √† la hausse jusqu'√† la fin des ann√©es 70 puis une l√©g√®re baisse et une stabilisation  
- il ne semble pas y avoir de cycles  
- en revanche, il y a bien une saisonnalit√© avec un "motif" annuel qui se r√©p√®te contenant un pic haut (parfois un 2√®me identifiable) et un creux  

Voil√† quelques exemples d'analyse graphique, et rapide, de s√©ries temporelles. Comme toujours, le code ayant g√©n√©r√© ce notebook est disponible sur le <a href="https://github.com/Statoscop/notebooks-blog" target="_blank">github de Statoscop</a>. Maintenant, la question est de savoir comment on va pouvoir identifier les diff√©rents √©l√©ments composant une s√©rie avant de s'int√©resser √† leur mod√©lisation et leur pr√©diction. Alors, impatients de savoir comment d√©composer une s√©rie temporelle ? La suite dans le prochain post !
  
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>  