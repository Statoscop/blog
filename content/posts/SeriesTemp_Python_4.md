Title: Les s√©ries temporelles avec Python (4/4) - Analyse et mod√©lisation
Author: Louis
Date: '2022-11-14'
Slug: timeseries-4
Category: Python, S√©ries Temporelles
Tags: Python, Machine Learning, Statistiques, Data Science, S√©ries temporelles, Datetime, ARMA, SARIMA, Time Series
Cover: images/cover_13.png
Summary: Analyse d'une s√©rie temporelle par d√©composition et mod√©lisations classiques.

[TOC]

>1. Introduction √† la manipulation de donn√©es temporelles avec Python
>2. Visualisation et op√©rations sur les s√©ries temporelles
>3. √âl√©ments th√©oriques et exemples
>4. **Analyse, mod√©lisation et pr√©diction**

Apr√®s une toute petite ellipse temporelle de plus d'un an, ce quatri√®me post s'attaque au vrai probl√®me √† savoir la d√©composition, la mod√©lisation et la pr√©diction des s√©ries temporelles √† l'aide des mod√®les statistiques classiques.

# D√©composition d'une s√©rie temporelle

Le principe de la d√©composition des s√©ries temporelles est de d√©couper les s√©ries originales en plusieurs composants ind√©pendants (vus dans le post pr√©c√©dent) : tendance, saisonnalit√©, cycle et bruit.

Pour illustrer tout √ßa, on va utiliser des donn√©es qui sont aux s√©ries temporelles ce que le dataset "Titanic" est au machine learning : la s√©rie AirPassenger qui mesure le nombre mensuel de passagers a√©riens, en milliers, de janvier 1949 √† d√©cembre 1960. Dans la suite, on notera $X_{t}$  la s√©rie AirPassengers et $Y_t=ln(X_t)$. 


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import fonctions as fct
```


```python
x = pd.read_csv('data/AirPassengers.csv', index_col=0, parse_dates=True)
y = np.log(x)

# cr√©ation d'un graphique avec la s√©rie originale
fig, ax = plt.subplots()
ln1 = ax.plot(x, c='r', label="nb passagers")

# cr√©ation d'un graphique avec la s√©rie log en conservent le m√™me axe des abscisses
ax2 = ax.twinx()
ln2 = ax2.plot(y, c='b', label="log(nb passagers)")

# ajout de la l√©gende
lns = ln1 + ln2
labels=[l.get_label() for l in lns]
ax.legend(lns, labels);
```


    
![Pelican](../images/SeriesTemp4/output_4_0.png)
    


On voit sur la s√©rie initiale (en rouge) que l'amplitude des variations augmente chaque ann√©e de mani√®re exponentielle. Or dans un cadre th√©orique parfait, on voudrait une variance constante au court du temps. Pour s'en approcher on peut passer par la fonction logarithme. En effet, comme $log(ab) = log(a) + log(b)$, le $log$ transforme un mod√®le **multiplicatif** en un mod√®le **additif**. Ainsi l'accroissement de l'amplitude des pics (donc de la variance) pour la s√©rie $X$ dispara√Æt avec la transformation logarithmique. C'est visible sur la courbe bleue.  

## La tendance

On a pr√©sent√© dans le premier post une fa√ßon de visualiser la tendance en lissant la courbe √† l'aide des moyennes mobiles sur une fen√™tre glissante d'observations. Lorsqu'on choisit diff√©rentes tailles de fen√™tres glissantes, on obtient les r√©sultats suivants.


```python
win = [3, 6, 9, 12, 15, 18, 21, 24, 36]
fig = plt.figure(figsize=(12,8))

for i in range(len(win)):
    ax = fig.add_subplot(3, 3, i+1)
    ax.plot(y)
    ax.plot(y.rolling(win[i], center=True).mean(), label=f"moyenne sur {win[i]} mois")
    ax.legend()
```


    
![Pelican](../images/SeriesTemp4/output_6_0.png)
    


En prenant une fen√™tre de 12, 24 ou 36 mois, une tendance lin√©aire et croissante est bien visible. Par ailleurs, le fait que cette tendance est identifiable pour des fen√™tres glissantes dont la taille est un multiple de 12 mois sugg√®re une notion de saisonnalit√©. On ne sait peut-√™tre pas encore ce que c'est, mais en tout cas, il se passe quelque chose tous les 12 mois...

## La saisonnalit√©

La saisonnalit√© d√©signe les tendances hebdomadaires, mensuelles, trimestrielles ou annuelles. Ce sont les motifs qui se r√©p√®tent sur une dur√©e infra-annuelle. Il semble qu'on ait ici une saisonnalit√© annuelle. Une mani√®re visuelle de v√©rifier l'effet saisonnier est de tracer une courbe pour chaque ann√©e en prenant les mois comme abscisses et visualiser ainsi la r√©partition des passagers au cours de chaque ann√©e.


```python
plt.figure(figsize=(12, 6))

yrs = [str(yr) for yr in y.index.year.unique()]
mths = y.index.month_name().unique()
colors = plt.cm.inferno(np.linspace(0,1,len(yrs)))

for i in range(len(yrs)):
    plt.plot(mths, y.loc[yrs[i]], label=yrs[i], color=colors[i])
    plt.legend(bbox_to_anchor=(1,1))
```


    
![Pelican](../images/SeriesTemp4/output_8_0.png)
    


√áa confirme bien l'intuition : les variations du nombre de passagers a√©riens sur une ann√©e sont similaires d'une ann√©e sur l'autre. C'est la saisonnalit√©. Et en plus c'est joli.

## Le bruit

Il ne reste plus qu'√† identifier ce qu'il reste : le bruit. On l'obtient donc en soustrayant (dans un mod√®le additif !) √† la s√©rie originale les tendance et saisonnalit√©. Au final, voil√† une s√©rie d√©compos√©e.


```python
from statsmodels.tsa.seasonal import seasonal_decompose, STL

decomp = seasonal_decompose(y, model='additive')

fig = decomp.plot()
fig.set_size_inches((10, 6))
fig.tight_layout()
plt.show()
```


    
![Pelican](../images/SeriesTemp4/output_10_0.png)
    
 
> üëã Nous c'est Antoine et Louis de Statoscop, une coop√©rative de statisticiens / data scientists.
> Vous voulez en savoir plus sur ce que l'on fait?
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>
<br>

# Pr√©vision avec un lissage exponentiel

Le lissage exponentiel est une technique de lissage pas br√©silien qui ressemble un peu aux moyennes mobiles. Cette m√©thode peut √™tre utilis√©e pour pr√©dire une s√©rie temporelle. C'est en fait un cas particulier des mod√®les ARMA d√©j√† √©voqu√©s. On en distingue diff√©rents types (simple, double, triple...). On notera que les lissages exponentiels simple et double ne sont plus vraiment d'actualit√© donc on pr√©sente seulement le lissage exponentiel triple qui, lui, est parfois utilis√©.

Le lissage exponentiel triple ou m√©thode de Holt-Winters est adapt√© √† des s√©ries √† tendance localement lin√©aire avec en plus une composante saisonni√®re. Ce qui est le cas de notre s√©rie logarithmique.


```python
from statsmodels.tsa.api import ExponentialSmoothing

let = ExponentialSmoothing(y.values, seasonal_periods=12, trend='add', seasonal='add').fit()

y_pred = let.forecast(60)# pr√©diction pour la s√©rie log
x_pred = np.exp(let.forecast(60)) # r√©cup√©ration des pr√©diction pour la s√©rie originale

fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))

ax1.plot(y, label='log(Airpass)')
ax1.plot(pd.date_range(y.index[len(y)-1], periods=60, freq='M'), y_pred, label='Pred')
ax1.legend()

ax2.plot(x, label='Airpass')
ax2.plot(pd.date_range(y.index[len(y)-1], periods=60, freq='M'), x_pred, label='Pred')
ax2.legend()

plt.show()
```


    
![Pelican](../images/SeriesTemp4/output_12_0.png)
    


# Pr√©vision √† l'aide d'un mod√®le ARIMA

Un mod√®le autor√©fressif int√©gr√© √† moyennes mobiles (ARIMA) est une g√©n√©ralisation du mod√®le ARMA, aper√ßu dans le post pr√©c√©dent. Ils sont utilis√©s dans certains cas o√π les donn√©es montrent une non-stationnarit√©. Une √©tape de diff√©renciation initiale (correspondant √† la partie "int√©gr√©e" du mod√®le) peut alors √™tre appliqu√©e une ou plusieurs fois afin d'√©liminer la non-stationnarit√©. Dans le mod√®le de base, trois param√®tres $(p,d,q)$ sont utilis√©s pour caract√©riser les mod√®les ARIMA. On les note donc $ARIMA(p,d,q)$ et ils sont d√©finis par :
$$
\left(1 - \sum_{i=1}^p \phi_i L^i \right) (1 - L)^d y_t = \mu + \left(1 + \sum_{i=1}^q \theta_i L^i \right) \varepsilon_t
$$

$L$ est l'op√©rateur *retard* qui consiste √† d√©caler la s√©rie d'une observation.

SARIMA, ou ARIMA saisonnier, est une g√©n√©ralisation du mod√®le ARIMA qui permet de tenir compte de la saisonnalit√©. Ce mod√®le est caract√©ris√© non plus par 3 param√®tres $(p,d,q)$ mais par 7 ! Il est not√© $SARIMA(p,d,q)(P,D,Q)_s$ o√π :
- $s$ est la p√©riode de la saisonnalit√©
- $(P,D,Q)$ sont les ordres de la partie saisonni√®re

## Stationnarisation de la s√©rie

Les s√©ries ont g√©n√©ralement une tendance et une saisonnalit√© toutefois afin de pouvoir les mod√©liser par des processus stationnaires comme les ARMA, il faut qu'elles soient stationnaires. Or elles ne le sont que rarement, il faut donc les stationnariser.

On sait que notre s√©rie n'est pas stationnaire (c'est v√©rifiable en regardant l'autocorr√©logramme qui doit pr√©senter une d√©croissance lente vers 0). Par ailleurs, on a d√©j√† identifi√© la saisonnalit√© de nos observations qui est annuelle. On peut d√©j√† effectuer une diff√©renciation en appliquant l'op√©rateur $(1-L^{12})$ .


```python
y_diff_12 = y - y.shift(12)
fct.plot_acf_pacf(y_diff_12[12:], fig_size=(9,5))
```


    
![Pelican](../images/SeriesTemp4/output_14_0.png)
    


La sortie ACF pr√©sente bien une d√©croissance lente vers 0, ce qui traduit la non-stationnarit√©. On effectue donc une diff√©renciation en appliquant cette fois l'op√©rateur $(1-L)$.


```python
y_diff_12_1 = y_diff_12 - y_diff_12.shift(1)
fct.plot_acf_pacf(y_diff_12_1[13:], fig_size=(9,5))
```


    
![Pelican](../images/SeriesTemp4/output_16_0.png)
    


Avec cette double diff√©renciation, on s'approche d'un autocorr√©logramme simple empirique. On va donc mod√©liser la s√©rie $(1-L)(1-L^{12})ln(X_t)$ par un mod√®le ARMA.

## Identification, estimation et validation de mod√®les

On va s'appuyer sur les sorties ACF et PACF donc on peut utiliser une des fonctions disponibles dans `fonctions.py`.


```python
fct.ts_plot(y_diff_12_1[13:].Airpass, fig_size=(10,7))
```


    
![Pelican](../images/SeriesTemp4/output_19_0.png)
    


Les graphiques ACF et PACF permettent de voir empiriquement qu'on va avoir un mod√®le $SARIMA(p,d,q)(P,D,Q)_s$ avec $s=12$.


Il est possible de trouver empiriquement un ordre de grandeur maximum pour les parties *autor√©gressive* et *moyenne mobile*, c'est-√†-dire pour les valeurs $p_{max}$ et $q_{max}$ d'un mod√®le $ARMA(p,q)$. La d√©termination de bornes sup√©rieures concernant l'ordre de la partie $AR$ et de la partie $MA$ passe par l'√©tude des autocorr√©logrammes et des autocorr√©logrammes partiels. En effet, si la s√©rie stationnaire suit un mod√®le $ARMA(p,q)$ alors :  

- l'autocorr√©logramme ACF devient nul √† partir du rang $q+1$  
- l'autocorr√©logramme partiel PACF devient nul √† partir du rang $p+1$

Le tableau suivant r√©sume les propri√©t√©s des trac√©s de l'ACF et de la PACF empiriques :

|      |   MA(q)    |    AR(p)   | ARMA(p,q) |
|------|:----------:|:----------:|:----------:|
| ACF  | 0 si k > q | $\searrow$ | $\searrow$ |
| PACF | $\searrow$ | 0 si k > p | $\searrow$ |

D√®s lors, en se basant sur les autocorr√©logrammes, il est possible de d√©terminer empiriquement les ordres maximum en regardant √† partir de quel rang les autocorr√©logrammes sont statistiquement nuls. Dans notre cas, on peut consid√©rer que $p_{max} = q_{max} = 1$ et on va donc tester diff√©rents mod√®les compte tenu de ces bornes.


**Mod√®le 1 :** pour commencer on mod√©lise un $SARIMA(1,1,1)(1,1,1)_{12}$ qui s'√©crit :
$$(1-\phi_{1}L)(1-\phi'_{1}L^{12})(1-L)(1-L^{12})ln(X_{t})=(1+\theta_{1}L)(1+\theta'_{1}L^{12})\epsilon_{t} $$


```python
from statsmodels.tsa.statespace.sarimax import *

m1 = SARIMAX(y.values, order=(1,1,1), seasonal_order=(1,1,1,12))
res1 = m1.fit(disp=False)
print(res1.summary())
```

                                         SARIMAX Results                                      
    ==========================================================================================
    Dep. Variable:                                  y   No. Observations:                  144
    Model:             SARIMAX(1, 1, 1)x(1, 1, 1, 12)   Log Likelihood                 245.152
    Date:                            Wed, 09 Nov 2022   AIC                           -480.304
    Time:                                    15:47:19   BIC                           -465.928
    Sample:                                         0   HQIC                          -474.462
                                                - 144                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ar.L1          0.1701      0.212      0.801      0.423      -0.246       0.586
    ma.L1         -0.5649      0.184     -3.074      0.002      -0.925      -0.205
    ar.S.L12      -0.0997      0.197     -0.506      0.613      -0.486       0.286
    ma.S.L12      -0.4967      0.210     -2.364      0.018      -0.909      -0.085
    sigma2         0.0013      0.000      8.458      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                 3.51
    Prob(Q):                              0.91   Prob(JB):                         0.17
    Heteroskedasticity (H):               0.61   Skew:                            -0.01
    Prob(H) (two-sided):                  0.11   Kurtosis:                         3.80
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).


Ce mod√®le a des coefficients non significatifs, on en teste un second en retirant dans le param√®tre associ√© √† la plus grande p-value : ici c'est le terme autor√©gressif saisonnier.

**Mod√®le 2 :** on mod√©lise donc un $SARIMA(1,1,1)(0,1,1)_{12}$ qui s'√©crit :
$$(1-\phi_{1}L)(1-L)(1-L^{12})ln(X_{t})=(1+\theta_{1}L)(1+\theta'_{1}L^{12})\epsilon_{t} $$


```python
m2 = SARIMAX(y.values, order=(1,1,1), seasonal_order=(0,1,1,12))
res2 = m2.fit(disp=False)
print(res2.summary())
```

                                         SARIMAX Results                                      
    ==========================================================================================
    Dep. Variable:                                  y   No. Observations:                  144
    Model:             SARIMAX(1, 1, 1)x(0, 1, 1, 12)   Log Likelihood                 244.945
    Date:                            Wed, 09 Nov 2022   AIC                           -481.890
    Time:                                    15:47:21   BIC                           -470.389
    Sample:                                         0   HQIC                          -477.217
                                                - 144                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ar.L1          0.1841      0.201      0.915      0.360      -0.210       0.578
    ma.L1         -0.5667      0.174     -3.248      0.001      -0.909      -0.225
    ma.S.L12      -0.5633      0.104     -5.417      0.000      -0.767      -0.359
    sigma2         0.0013      0.000      8.624      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):                 3.35
    Prob(Q):                              0.89   Prob(JB):                         0.19
    Heteroskedasticity (H):               0.60   Skew:                            -0.00
    Prob(H) (two-sided):                  0.09   Kurtosis:                         3.78
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).


Ce mod√®le a toujours un coefficient non significatif, on en teste un 3√®me mod√®le en retirant le terme autor√©gressif d'ordre 1.

**Mod√®le 3 :** on mod√©lise donc un $SARIMA(0,1,1)(0,1,1)_{12}$ qui s'√©crit :
$$(1-L)(1-L^{12})ln(X_{t})=(1+\theta_{1}L)(1+\theta'_{1}L^{12})\epsilon_{t} $$


```python
m3 = SARIMAX(y.values, order=(0,1,1), seasonal_order=(0,1,1,12))
res3 = m3.fit(disp=False)
print(res3.summary())
```

                                         SARIMAX Results                                      
    ==========================================================================================
    Dep. Variable:                                  y   No. Observations:                  144
    Model:             SARIMAX(0, 1, 1)x(0, 1, 1, 12)   Log Likelihood                 244.696
    Date:                            Wed, 09 Nov 2022   AIC                           -483.393
    Time:                                    15:47:22   BIC                           -474.767
    Sample:                                         0   HQIC                          -479.888
                                                - 144                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ma.L1         -0.4013      0.073     -5.496      0.000      -0.544      -0.258
    ma.S.L12      -0.5566      0.096     -5.780      0.000      -0.745      -0.368
    sigma2         0.0013      0.000      9.121      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (L1) (Q):                   0.04   Jarque-Bera (JB):                 1.90
    Prob(Q):                              0.85   Prob(JB):                         0.39
    Heteroskedasticity (H):               0.58   Skew:                             0.02
    Prob(H) (two-sided):                  0.07   Kurtosis:                         3.59
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).


Cette fois tous les coefficients sont significatifs, il faut encore tester ce qu'on appelle la "blancheur" des r√©sidus : on veut avoir des r√©sidus qui sont un bruit blanc (faible), c'est-√†-dire, entre autres qu'ils sont non corr√©l√©s.  
Pour √ßa on peut utiliser le test de Ljung-Box d'autocorr√©lation des r√©sidus qui est impl√©ment√© dans la fonction `acorr_ljungbox` de `statsmodels.stats.diagnostic` 


```python
from statsmodels.stats.diagnostic import acorr_ljungbox
print('Retard : p-value')
for lag in [6, 12, 18, 24, 30, 36]:
    print('{} : {}'.format(lag, acorr_ljungbox(res3.resid, lags=lag, return_df=False)[1].mean()))
```

    Retard : p-value
    6 : 0.981720860200859
    12 : 0.9083064612785483
    18 : 0.6207396507474344
    24 : 0.5187361239841579
    30 : 0.5161804687133994
    36 : 0.560395354197207


Toutes les p-values sont √©lev√©es ce qui signifie qu'on ne peut pas rejeter l'hypoth√®se nulle du test. Or, l'hypoth√®se $H_0$ est qu'il n'y a pas d'autocorr√©lation des r√©sidus, on peut pas la rejeter, donc on est bon !

On peut enfin tester la normalit√© des r√©sidus avec le test de Shapiro-Wilk impl√©ment√© dans la fonction `scipy.stats.shapiro`, l'hypoth√®se nulle √©tant que les √©tant que les observations sont normalement distribu√©es, on souhaite encore une fois une p-value √©lev√©e pour ne pas rejeter $H_0$.


```python
from scipy.stats import shapiro
shapiro(res3.resid[13:])
```




    ShapiroResult(statistic=0.9908484220504761, pvalue=0.547280490398407)



Le test de normalit√© est √©galement valid√© pour ce mod√®le et on peut afficher les r√©sidus pour le visualiser :


```python
res3.plot_diagnostics(figsize=(10, 8));
```


    
![Pelican](../images/SeriesTemp4/output_31_0.png)
    


## Pr√©vision de l‚Äôann√©e 1961 avec le mod√®le retenu

Maintenant qu'on a mod√©lis√© notre s√©rie, on peut se servir de ce mod√®le pour faire des pr√©dictions avec la m√©thode `get_forecast()`


```python
# pr√©diction et intervalles de confiance
y_pred = res3.get_forecast(12)
x_pred = np.exp(y_pred.predicted_mean)
x_pred_inf = [np.exp(p[0]) for p in y_pred.conf_int(alpha=0.05)]
x_pred_sup = [np.exp(p[1]) for p in y_pred.conf_int(alpha=0.05)]

# affichage
plt.figure(figsize=(14,6))
plt.plot(x.index, x.values, label='Passagers')
plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred, color='r', label='Predit')
plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred_inf, color='g', linestyle='--')
plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred_sup, color='g', linestyle='--')
plt.legend()
plt.show()
```


    
![Pelican](../images/SeriesTemp4/output_33_0.png)
    


## √âvaluation de la qualit√© pr√©dictive du mod√®le

On n'en a pas parl√© jusqu'ici mais pour construire un mod√®le pr√©dictif, il faut mettre en place, entra√Æner, tester et √©valuer votre mod√®le. Pour cela, il est n√©cessaire de d√©couper nos donn√©es en √©chantillons d'entra√Ænement et de test. Pour des observations non chronologiques, on d√©coupe de mani√®re al√©atoire mais pour des donn√©es temporelles, on est oblig√© de conserver la chronologie dans l'√©chantillon d'entra√Ænement. Pour cela, on va tout simplement tronquer la s√©rie : les **donn√©es d'entra√Ænement sont les observations jusqu'en 1958** et les **donn√©es de test sont celles des ann√©es 1959 et 1960**.

On va utiliser directement le mod√®le $SARIMA(0,1,1)(0,1,1)_{12}$ obtenu avec toutes les observations de la s√©rie. Toutefois, il est plus rigoureux de s'assurer que le mod√®le retenu pr√©c√©demment est toujours valide sur la s√©rie tronqu√©e `y_train`. Pour cela on regarde la significativit√© des coefficients, l'autocorr√©lation et la normalit√© des r√©sidus.

Le mod√®le $SARIMA(0,1,1)(0,1,1)_{12}$ √©tant bien valide sur les donn√©es tronqu√©es, on peut pr√©dire les valeurs des ann√©es 1959 et 1960 et les afficher sur le graphique suivant avec les valeurs r√©elles, pr√©dites et les intervalles de confiance.

Enfin, on ajoute des m√©triques d'erreur : RMSE et MAPE.


```python
# s√©rie x : s√©rie des passagers
x_train = x.loc[:'1958', 'Airpass']
x_test = x.loc['1959':, 'Airpass']
# s√©rie y=log(x) : transformation logarithmique de la s√©rie des passagers 
y_train = y.loc[:'1958', 'Airpass']
y_test = y.loc['1959':, 'Airpass']

# mod√©lisation et summary
model = SARIMAX(y_train.values, order=(0,1,1), seasonal_order=(0,1,1,12))
res = model.fit(disp=False)
print(res.summary())

# test d'autocorr√©lation des r√©sidus
print('Retard : p-value')
for lag in [6, 12, 18, 24, 30, 36]:
    print('{} : {}'.format(lag, acorr_ljungbox(res.resid, lags=lag, return_df=False)[1].mean()))
    
# test de normalit√© des r√©sidus
print(shapiro(res.resid[13:]))

# pr√©diction et intervalles de confiance
y_pred = res.get_forecast(len(y_test))
x_pred = np.exp(y_pred.predicted_mean)
x_pred_inf = [np.exp(p[0]) for p in y_pred.conf_int(alpha=0.05)]
x_pred_sup = [np.exp(p[1]) for p in y_pred.conf_int(alpha=0.05)]

# graphique
plt.figure(figsize=(14,6))
plt.plot(x_test.index, x_test.values, label='Passagers')
plt.plot(x_test.index, x_pred, color='r', label='Predit')
plt.plot(x_test.index, x_pred_inf, color='g', linestyle='--')
plt.plot(x_test.index, x_pred_sup, color='g', linestyle='--')
plt.legend()
plt.show()

# m√©triques
rmse = np.sqrt(((x_test-x_pred)**2).mean())
mape = (np.abs(1-x_pred/x_test)).mean()*100
print("RMSE = {} et MAPE = {}".format(rmse, mape))
```

                                         SARIMAX Results                                      
    ==========================================================================================
    Dep. Variable:                                  y   No. Observations:                  120
    Model:             SARIMAX(0, 1, 1)x(0, 1, 1, 12)   Log Likelihood                 197.505
    Date:                            Wed, 09 Nov 2022   AIC                           -389.010
    Time:                                    15:47:24   BIC                           -380.991
    Sample:                                         0   HQIC                          -385.759
                                                - 120                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ma.L1         -0.3422      0.087     -3.921      0.000      -0.513      -0.171
    ma.S.L12      -0.5407      0.105     -5.158      0.000      -0.746      -0.335
    sigma2         0.0014      0.000      7.867      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                 0.94
    Prob(Q):                              0.92   Prob(JB):                         0.63
    Heteroskedasticity (H):               0.37   Skew:                             0.12
    Prob(H) (two-sided):                  0.00   Kurtosis:                         3.39
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).
    Retard : p-value
    6 : 0.9843609885289526
    12 : 0.9115568666108983
    18 : 0.6472654644765928
    24 : 0.5846032458884963
    30 : 0.6111276421562357
    36 : 0.6613716515087891
    ShapiroResult(statistic=0.9883859157562256, pvalue=0.48699426651000977)



    
![Pelican](../images/SeriesTemp4/output_35_1.png)
    


    RMSE = 43.18024430485034 et MAPE = 8.51556515346472


Ainsi on obtient su les 2 ann√©es 1959 et 1960, une erreur moyenne de pr√©diction de 43,2 passagers (RMSE) et une erreur moyenne absolue en pourcentage de 8,5%.

## Une autre approche pour la d√©termination du mod√®le

Pour ajuster un mod√®le SARIMA aux donn√©es d'une s√©rie temporelle, il faut d√©terminer le jeu de param√®tres $(p,d,q)(P,D,Q)s$ optimaux. On peut faire de la recherche sur grille qui consiste √† tester de mani√®re it√©rative plusieurs valeurs possibles des param√®tres et d'√©valuer les mod√®les en utilisant des crit√®res tels que AIC ou BIC.

On utilisera nous le crit√®re d'information Akaike ($AIC$) qui mesure la qualit√© relative des mod√®les statistiques pour un ensemble d'observations donn√©. √Ä partir d'un ensemble de mod√®les, le crit√®re $AIC$ permet d'estimer la qualit√© de chacun des mod√®les par rapport aux autres. Cela permet donc de s√©lectionner des mod√®les. Intuitivement, le crit√®re $AIC$ mesure le compromis entre la qualit√© de l'ajustement du mod√®le aux observations fournies et la complexit√© du mod√®le (nombre de param√®tres inclus et estim√©s). Le crit√®re $AIC$ est calcul√© par :

$AIC=2k-2ln(L)$ avec :
- $k$ nombre de param√®tres estim√©s
- $L$ valeur maximale de la vraisemblance (pour simplifier maximiser la vraisemblance revient plus ou moins √† minimiser l'erreur)

Le meilleur mod√®le est celui qui minimise le crit√®re $AIC$. √Ä titre de comparaison, on regarde aussi selon le crit√®re BIC.

Dans on premier temps, on va g√©n√©rer toutes les valeurs possibles des param√®tres que nous aimerions √©valuer, √† savoir $(p,d,q)(P,D,Q)$ puisqu'on sait d√©j√† que $s=12$. On testera les mod√®les pour toutes les combinaisons possibles pour des valeurs entre 0 et 1 pour chacun des 6 param√®tres c'est-√†-dire $2^6 = 64$ possibilit√©s. Si on va jusqu'√† 2 pour chaque param√™tre, on passe √† $3^6 = 729$ possibilit√©s, ce qui va commencer √† faire un peu long...


```python
from statsmodels.tsa.statespace.sarimax import *
from tqdm import tqdm
import itertools

import warnings
warnings.filterwarnings(action='ignore') #'once' pour afficher le warning une seule fois

# on g√©n√®re toutes les combinaisons possibles des param√®ters
p = d = q = P = D = Q = range(2)
params = list(itertools.product(p, d, q, P, D, Q))

# on initialise nos param√®tres optimaux
tmp_model = None
best = {}
best['aic'] = {'pdq': None, 'PDQ':None, 'model':None, 'aic':np.inf, 'bic':np.inf}
best['bic'] = {'pdq': None, 'PDQ':None, 'model':None, 'aic':np.inf, 'bic':np.inf}

for p,d,q,P,D,Q in tqdm(params):
    tmp_model = SARIMAX(y_train,
                        order = (p,d,q),
                        seasonal_order = (P,D,Q,12),
                        enforce_stationarity=True,
                        enforce_invertibility=True)
    res = tmp_model.fit(disp=False)

    if res.aic < best['aic']['aic']:
        best['aic']['aic'] = res.aic
        best['aic']['bic'] = res.bic
        best['aic']['pdq'] = (p,d,q)
        best['aic']['PDQ'] = (P,D,Q)
        best['aic']['model'] = res
    
    if res.bic < best['bic']['bic']:
        best['bic']['aic'] = res.aic
        best['bic']['bic'] = res.bic
        best['bic']['pdq'] = (p,d,q)
        best['bic']['PDQ'] = (P,D,Q)
        best['bic']['model'] = res

print(f"Mod√®le retenu selon le crit√®re AIC : SARIMA{best['aic']['pdq']}{best['aic']['PDQ']}12 - AIC:{best['aic']['aic']} - BIC:{best['aic']['bic']}")
print(f"Mod√®le retenu selon le crit√®re BIC : SARIMA{best['bic']['pdq']}{best['bic']['PDQ']}12 - AIC:{best['bic']['aic']} - BIC:{best['bic']['bic']}")
```

    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:59<00:00,  1.08it/s]

    Mod√®le retenu selon le crit√®re AIC : SARIMA(0, 1, 1)(1, 0, 1)12 - AIC:-405.4220890824692 - BIC:-394.3055951100231
    Mod√®le retenu selon le crit√®re BIC : SARIMA(0, 1, 1)(1, 0, 1)12 - AIC:-405.4220890824692 - BIC:-394.3055951100231


    


Le meilleur mod√®le selon le crit√®re AIC est le $SARIMA(0,1,1)(1,0,1)_{12}$ qui diff√®re l√©g√®rement de celui obtenu avec la m√©thode pr√©c√©dente. Il convient de noter cependant que le mod√®le pr√©c√©dent $SARIMA(0,1,1)(0,1,1)_{12}$ a √©t√© obtenu sur toutes les observations.

Passons √† la pr√©diction pour terminer.


```python
# mod√®le retenu
res = best['aic']['model']

# summary
print(res.summary())

# r√©sidus
res.plot_diagnostics(figsize=(14, 10));
print("Test de Shapiro :", shapiro(res.resid[13:]))

# pr√©diction et intervalles de confiance
y_pred = res.get_prediction(start=pd.to_datetime('1958-12-01'), end=pd.to_datetime('1960-12-01'))
x_pred = np.exp(y_pred.predicted_mean)
pred_CI = np.exp(y_pred.conf_int())

# affichage
ax = x['1949':].plot(label='S√©rie observ√©e', color='darkturquoise', figsize=(14,6))
x_pred.plot(ax=ax, label='Pr√©visions', color='deeppink')

# affichage de l'intervalle de confiance
ax.fill_between(pred_CI.index,
                pred_CI.iloc[:, 0],
                pred_CI.iloc[:, 1], color='plum', alpha=.25)

# un peu de style
ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('1958-12-01'), x.index[-1], alpha=.1, zorder=-1, color='grey')
ax.set_xlabel('Date')
ax.set_ylabel('Passagers')
plt.legend(loc='upper left')
plt.show()

# m√©triques
rmse = np.sqrt(((x_test-x_pred)**2).mean())
mape = (np.abs(1-x_pred/x_test)).mean()*100
print("RMSE = {} et MAPE = {}".format(rmse, mape))
```

                                         SARIMAX Results                                      
    ==========================================================================================
    Dep. Variable:                            Airpass   No. Observations:                  120
    Model:             SARIMAX(0, 1, 1)x(1, 0, 1, 12)   Log Likelihood                 206.711
    Date:                            Wed, 09 Nov 2022   AIC                           -405.422
    Time:                                    15:48:24   BIC                           -394.306
    Sample:                                01-01-1949   HQIC                          -400.908
                                         - 12-01-1958                                         
    Covariance Type:                              opg                                         
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    ma.L1         -0.2909      0.082     -3.544      0.000      -0.452      -0.130
    ar.S.L12       0.9888      0.009    110.418      0.000       0.971       1.006
    ma.S.L12      -0.5472      0.107     -5.121      0.000      -0.757      -0.338
    sigma2         0.0014      0.000      7.899      0.000       0.001       0.002
    ===================================================================================
    Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):                 0.46
    Prob(Q):                              0.89   Prob(JB):                         0.79
    Heteroskedasticity (H):               0.43   Skew:                             0.11
    Prob(H) (two-sided):                  0.01   Kurtosis:                         3.22
    ===================================================================================
    
    Warnings:
    [1] Covariance matrix calculated using the outer product of gradients (complex-step).
    Test de Shapiro : ShapiroResult(statistic=0.988020658493042, pvalue=0.459557443857193)



    
![Pelican](../images/SeriesTemp4/output_39_1.png)
    



    
![Pelican](../images/SeriesTemp4/output_39_2.png)
    


    RMSE = 47.11786986382567 et MAPE = 9.220540274482799


Le RMSE et le MAPE sont l√©g√®rement plus √©lev√©s qu'avec le pr√©c√©dent mod√®le mais restent dans les m√™mes ordres de grandeur. On notera donc que dans ce cas pr√©cis, l'approche par crit√®re d'information ne permet pas d'avoir les meilleurs r√©sultats de pr√©diction sur les ann√©es 1959 et 1960. Toutefois l'approche √©tant plus exhaustive puisqu'on compare un grand nombre de mod√®le, elle m√©rite d'√™tre conserv√©e. Cette r√©flexion entre dans le cadre de la s√©lection de mod√®les si vous voulez en savoir plus.

# Pour aller plus loin

On va s'arr√™ter l√† pour cet article qui est d√©j√† bien assez long mais si vous souhaitez creuser un peu sur les s√©ries temporelles, voici quelques pistes de recherche :  
- la **coint√©gration** : lorsqu'une relation de long terme semble exister entre deux s√©ries  
- le **mod√®le √† correction d'erreur** et la notion associ√©e de vitesse d'ajustement  
- d'autres mod√®les adapt√©s adapt√©s aux s√©ries temporelles : **Prophet** ou encore les r√©seaux de neurones r√©ccurents avec notamment les **LSTM**. Tiens tiens, mais ce serait pas une bonne id√©e pour un petit "Bonus Series Temp" ? Allez d'accord !  

C'est la fin de cet article! N'h√©sitez pas √† [visiter notre site](https://www.statoscop.fr) et √† nous suivre sur [Twitter](https://twitter.com/stato_scop) et [Linkedin](https://www.linkedin.com/company/statoscop). Pour retrouver l'ensemble du code ayant servi √† g√©n√©rer cette note, vous pouvez vous rendre sur le [github de Statoscop](https://github.com/Statoscop/notebooks-blog).  

  
<div class = "d-flex justify-content-center mt-4">
   <a href="https://statoscop.fr" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Visiter notre site</a>
   <a href="https://statoscop.fr/contact" target=_blank class="btn btn-primary btn-custom text-uppercase" type="button">Nous contacter</a>
</div>